{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3687bd7",
   "metadata": {},
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bf9f4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "from random import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48158f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e66527",
   "metadata": {},
   "source": [
    "### 1. Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea7af283",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anushkaojha/2nd/NLP/NLP_Assignments/A4/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/anushkaojha/2nd/NLP/NLP_Assignments/A4/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['As for violence, where would art and literature be without it? The Book of Genesis could not go more than three chapters without its first murder. Bodies litter the stage in Shakespeare’s tragedies. Grimm fairy tales are grim indeed. In the American Film Institute’s lineup of the 100 greatest American movies of all time, at least 60 contain one form of brutality or another, much of it exceedingly bloody.\\n\\nOne film classic from 1949, which did not make the institute’s list, offers the proposition that violence and cultural achievement often go hand in hand. “You know what the fellow said,” a character in “The Third Man” says. “In Italy for 30 years under the Borgias, they had warfare, terror, murder and bloodshed, but they produced Michelangelo, Leonardo da Vinci and the Renaissance. In Switzerland, they had brotherly love. They had 500 years of democracy and peace. And what did that produce? The cuckoo clock.”\\n\\nThe advent of warning labels has hardly ended the artistic exploration of sex and violence. Notwithstanding Mr. Zappa’s dire forecasts, government repression does not reign. The last time anyone looked, songs were still being recorded and teenagers were still listening. An argument could even be made that these labels sometimes have an effect other than the intended one, by pointing teenagers, who practically live to upset their elders, in the direction of the forbidden fruit.\\n\\nAnd the labels keep coming. The latest, as Retro Report notes, are “trigger warnings” that have been proposed on some college campuses, to alert students to curriculum material that may upset them or possibly cause post-traumatic reactions in, say, rape victims or combat veterans. Works suggested for such advisories have included “The Great Gatsby” (misogyny), “The Merchant of Venice” (anti-Semitism) and “Mrs. Dalloway” (suicide). Are these alerts a reasonable way to shield the more vulnerable from harm, as proponents assert? Or are they, as critics fire back, a misguided notion that infantilizes students who would be better served by dealing directly with life’s harsher realities?\\n\\nWhen it comes to warnings, consistency is not always a helpmate. Susan Baker, for instance, told Retro Report that she still believes in the importance of labels for explicit music, but that she was skeptical about classroom trigger warnings. Whether one is right or left politically, much seems to depend on the target — on, if you will, whose ox is Gored.',\n",
       " 'Story highlights It is the farthest north the Russian spy ship has ventured\\n\\nThe vessel is outfitted with a variety of high-tech spying equipment\\n\\nWashington (CNN) A Russian spy ship sits 30 miles off the coast of Connecticut, a US defense official told CNN, while an armed Russian warplane recently carried out a \"mock attack\" against a US ship.\\n\\nThis is the farthest north the Russian spy vessel has ever ventured, according to US defense official.\\n\\nCNN reported that the Leonov, which conducted similar patrols in 2014 and 2015, was off the coast of Delaware Wednesday, but typically it only travels as far as Virginia.\\n\\nThe ship is based with Russia\\'s northern fleet on the North Sea but had stopped over in Cuba before conducting its patrol along the Atlantic Coast and is expected to return there following its latest mission.\\n\\nThe vessel is outfitted with a variety of high-tech spying equipment and is designed to intercept signals intelligence. The official said that the US Navy is \"keeping a close eye on it.\"\\n\\nRead More',\n",
       " 'Also popular this week: Christ Flix : Why U No Stick To The Bible?\\n\\nI came across this list of questions for atheists and thought I would answer them:\\n\\nHow would you define atheism? Atheism is a lack of belief in any gods. Do you act according to what you believe (there is no God) in or what you don’t believe in (lack belief in God)? How would one act according to a non-belief in god? Would I take time to not-pray? Maybe I’d spend Sunday morning standing outside a church? The answer to this idiotic question is no, I do not live my life based on the fact that there is no god. I simply live my life for me and my family, taking responsibility for my own actions. You cannot “act according to what you believe” when you don’t believe. It is a lack of belief. Lack. Do you think it is inconsistent for someone who “lacks belief” in God to work against God’s existence by attempting to show that God doesn’t exist? No, just like it was not inconsistent for someone who did not believe the earth was flat to work against the common belief the earth was flat by attempting to show the earth was, in fact, round. Living in truth is the key. How sure are you that your atheism properly represents reality? It’s not that I am sure there is no god, it is that I do not blindly believe in one with no evidence. Evidence is what gives us our reality. Without evidence, it’s fiction. Reality is aptly presented by facts. Not old stories. How sure are you that your atheism is correct? My lack in belief that there are any gods is a direct result of there being no evidence for them. I would change my mind about there being a god if there was evidence that one existed. It’s got nothing to do with being correct or incorrect. How would you define what truth is? Truth is represented by facts that are backed up with evidence. Why do you believe your atheism is a justifiable position to hold? Because there is no evidence for god. Are you a materialist or a physicalist or what? That has nothing to with atheism. Atheism is the lack of belief in a god. That’s it. Do you affirm or deny that atheism is a worldview? Why or why not? No, it’s not a worldview, because it is simply a lack of belief in a god. You can’t make an entire worldview out of that. An atheist can still believe in anything else, just not a god. They can believe psychics are real, they can believe the weather is caused by bejeweled dragons living in Mt. Everest. They can believe storks deliver babies. A worldview cannot be just a lack of belief in a god. Not all atheists are antagonistic to Christianity but for those of you who are, why the antagonism? Because religion is wrong. Teaching children to believe that this life is just a lead up to what comes after, is teaching them that this life is not the most valuable part of our existence. Believers blame their actions on forces outside of their control rather than taking responsibility for what they do. They feel god is in control and some people take that literally and, lacking faith in themselves, let their potential wither and die away while they pray for change. It also endorses hate, intolerance and denies scientific evidence. It has been the driving force behind brutal killings, genocides, executions, rapes, slavery, oppression and it has driven millions of people to suicide. It’s wrong, no matter which way you slice it. If you were at one time a believer in the Christian God, what caused you to deny his existence? I was never a believer. Do you believe the world would be better off without religion? Yes. Do you believe the world would be better off without Christianity? Yes. Do you believe that faith in a God or gods is a mental disorder? Absolutely not. Must God be known through the scientific method? No. If he appeared to me in my living room right now, I’d believe, even if I couldn’t prove it to anyone. If you answered yes to the previous question, then how do you avoid a category mistake by requiring material evidence for an immaterial God? N/A Do we have any purpose as human beings? Absolutely. There are many different possible purposes to each person’s life. It can be something completely subjective, like raising your kids to be great people or spending your life rescuing animals. It can also be something objective, like publishing research and works that will lead to massive change in our world that lasts well beyond a lifetime. If we do have purpose, can you as an atheist please explain how that purpose is determined? It is determined by the individual and in some cases it can also be determined by the world (such as in the case of Hippocrates or Herman Melville or Mark Twain or Galileo). Where does morality come from? My morality comes from my family and me. My mother and father taught me the difference between right and wrong. I have furthered those teachings through thought. I take full responsibility for my morality. I can, without the help of a book or a fictional character, understand right from wrong all on my own. Are there moral absolutes? Fuck yes. If there are moral absolutes, could you list a few of them? Rape is wrong. Murder is wrong. Violence is wrong. Teaching children that there is some old man in the sky judging our every move is wrong. Do you believe there is such a thing as evil? If so, what is it? No, I do not. I believe horrible shocking things come from messed up people who became messed up through previous trauma or through mental illness. If you believe that the God of the Old Testament is morally bad, by what standard do you judge that he is bad? I don’t believe in him, so I don’t believe he is one way or another. What would it take for you to believe in God? Evidence. What would constitute sufficient evidence for God’s existence? Seeing him, or having peer reviewed, independently reproduced findings that prove it. Must this evidence be rationally based, archaeological, testable in a lab, etc., or what? Yes to all. Do you think that a society that is run by Christians or atheists would be safer? Why? Atheists. Why? Because atheists take responsibility for their own actions, and they take action to make things better rather than pray. Do you believe in free will? (free will being the ability to make choices without coersion). Yes. If you believe in free will, do you see any problem with defending the idea that the physical brain, which is limited and subject to the neuro-chemical laws of the brain, can still produce free will choices? Again, nothing to do with atheism. Atheism is merely the lack of belief in a god. One atheist could believe the brain is run by My Little Ponies, another could think the brain is nothing but chemical reactions. Depends on the atheist. The only thing we have in common is a lack in belief of god. If you affirm evolution and that the universe will continue to expand forever, then do you think it is probable that given enough time, brains would evolve to the point of exceeding mere physical limitations and become free of the physical and temporal and thereby become “deity” and not be restricted by space and time? If not, why not? How does one lead to the other? No, I don’t think our brains will evolve to the point that they become a deity. Evolution does not mean things become more powerful. Evolution means adaptation to the environment. If surviving our environment required us to be less intelligent, evolution would lead to a weaker mind. If you answered the previous question in the affirmative, then aren’t you saying that it is probable that some sort of God exists? N/A\\n\\nAlso popular this week: Christ Flix : Why U No Stick To The Bible?\\n\\nEvery argument I have ever had with a creationist has gone the same way as this questionnaire: it just proves that they do not understand what atheism is. It’s absolutely impossible to have a coherent, civilized debate with a creationist until they understand what atheism is. I doubt that day will ever come.\\n\\nWhat are your answers to these questions? Feel free to post in the comments, or write your own blog post and send me the link : mommy@godlessmom.com .']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from itertools import islice\n",
    "from numpy.random import default_rng\n",
    "\n",
    "SEED = 1234\n",
    "rand = default_rng(SEED)\n",
    "\n",
    "ds_stream = load_dataset(\"Skylion007/openwebtext\", split=\"train\", streaming=True)\n",
    "\n",
    "# Take a small chunk first (e.g., 50k) then sample 10k from it\n",
    "buffer_size = 50_000\n",
    "buffer = list(islice(ds_stream, buffer_size))\n",
    "\n",
    "# Randomly sample 10k from buffer\n",
    "sample_idx = rand.choice(len(buffer), 10_000, replace=False)\n",
    "dataset_sample = [buffer[i] for i in sample_idx]\n",
    "\n",
    "# Extract texts\n",
    "texts = [ex[\"text\"] for ex in dataset_sample]\n",
    "texts[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f482e5",
   "metadata": {},
   "source": [
    "## Convert OpenWebText documents into a flat list of real sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3ac7cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 200000\n",
      "As for violence, where would art and literature be without it?\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def split_to_sentences(doc: str):\n",
    "    doc = doc.replace(\"\\n\", \" \").strip()\n",
    "    # Simple sentence split based on punctuation\n",
    "    sents = re.split(r\"(?<=[.!?])\\s+\", doc)\n",
    "    # Keep meaningful sentences (avoid super short noise)\n",
    "    sents = [s for s in sents if len(s.split()) >= 5]\n",
    "    return sents\n",
    "\n",
    "# texts = list of OpenWebText documents you sampled (e.g., 10k docs)\n",
    "all_sents = []\n",
    "for doc in texts:\n",
    "    if isinstance(doc, str) and len(doc.strip()) > 0:\n",
    "        all_sents.extend(split_to_sentences(doc))\n",
    "\n",
    "# OPTIONAL: cap number of sentences to keep it manageable\n",
    "all_sents = all_sents[:200_000]\n",
    "\n",
    "print(\"Total sentences:\", len(all_sents))\n",
    "print(all_sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e6fff8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept sentences: 199828\n",
      "As for violence, where would art and literature be without it?\n"
     ]
    }
   ],
   "source": [
    "sentences = [s.replace(\"\\n\", \" \") for s in all_sents]\n",
    "sentences = [s for s in sentences if len(s.split()) <= 200]  # keep shorter sentences for training stability\n",
    "\n",
    "print(\"Kept sentences:\", len(sentences))\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fcbef275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as for violence where would art and literature be without it\n"
     ]
    }
   ],
   "source": [
    "text = [s.lower() for s in sentences]\n",
    "text = [re.sub(r\"[.,!?\\-]\", \"\", s) for s in text]\n",
    "\n",
    "print(text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c830c936",
   "metadata": {},
   "source": [
    "4) Build vocab (word2id / id2word) with special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "002ecf31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating word2id: 100%|██████████| 196044/196044 [00:00<00:00, 3544075.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 196048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "word_list = list(set(\" \".join(text).split()))\n",
    "\n",
    "word2id = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
    "\n",
    "for i, w in tqdm(enumerate(word_list), total=len(word_list), desc=\"Creating word2id\"):\n",
    "    word2id[w] = i + 4\n",
    "\n",
    "id2word = {v: k for k, v in word2id.items()}\n",
    "vocab_size = len(word2id)\n",
    "\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060f7a91",
   "metadata": {},
   "source": [
    "5) Build token_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba32edeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences to token IDs: 100%|██████████| 199828/199828 [00:00<00:00, 237581.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[114742, 84692, 52426, 79853, 116219, 143067, 175482, 42225, 13199, 52736, 146747]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "token_list = []\n",
    "for sentence in tqdm(text, desc=\"Processing sentences to token IDs\"):\n",
    "    token_list.append([word2id[w] for w in sentence.split() if w in word2id])\n",
    "\n",
    "print(token_list[0][:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb259bc",
   "metadata": {},
   "source": [
    "### 3. Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "154e3832",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random, shuffle, randint, randrange\n",
    "\n",
    "batch_size = 6\n",
    "max_mask   = 20\n",
    "max_len    = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3bb5dfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch():\n",
    "    batch = []\n",
    "    positive = negative = 0\n",
    "\n",
    "    while positive != batch_size / 2 or negative != batch_size / 2:\n",
    "\n",
    "        tokens_a_index, tokens_b_index = randrange(len(sentences)), randrange(len(sentences))\n",
    "        tokens_a, tokens_b = token_list[tokens_a_index], token_list[tokens_b_index]\n",
    "\n",
    "        # 1) token embedding\n",
    "        input_ids = [word2id['[CLS]']] + tokens_a + [word2id['[SEP]']] + tokens_b + [word2id['[SEP]']]\n",
    "\n",
    "        # 2) segment embedding\n",
    "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        # truncate if too long\n",
    "        input_ids = input_ids[:max_len]\n",
    "        segment_ids = segment_ids[:max_len]\n",
    "\n",
    "        # 3) masking\n",
    "        n_pred = min(max_mask, max(1, int(round(len(input_ids) * 0.15))))\n",
    "\n",
    "        candidates_masked_pos = [\n",
    "            i for i, token in enumerate(input_ids)\n",
    "            if token != word2id['[CLS]'] and token != word2id['[SEP]']\n",
    "        ]\n",
    "        shuffle(candidates_masked_pos)\n",
    "\n",
    "        masked_tokens, masked_pos = [], []\n",
    "        for pos in candidates_masked_pos[:n_pred]:\n",
    "            masked_pos.append(pos)\n",
    "            masked_tokens.append(input_ids[pos])\n",
    "\n",
    "            # ✅ correct 80/10/10 using ONE random value\n",
    "            p = random()\n",
    "            if p < 0.8:  # 80% -> [MASK]\n",
    "                input_ids[pos] = word2id['[MASK]']\n",
    "            elif p < 0.9:  # 10% -> random token (avoid specials 0-3)\n",
    "                input_ids[pos] = randint(4, vocab_size - 1)\n",
    "            else:\n",
    "                pass  # 10% -> keep original\n",
    "\n",
    "        # 4) pad to max_len\n",
    "        n_pad = max_len - len(input_ids)\n",
    "        input_ids.extend([word2id['[PAD]']] * n_pad)\n",
    "        segment_ids.extend([0] * n_pad)\n",
    "\n",
    "        # 5) pad masked tokens/pos to max_mask\n",
    "        if max_mask > n_pred:\n",
    "            n_pad = max_mask - n_pred\n",
    "            masked_tokens.extend([0] * n_pad)\n",
    "            masked_pos.extend([0] * n_pad)\n",
    "\n",
    "        # 6) NSP label (now valid because sentences are in order)\n",
    "        if tokens_a_index + 1 == tokens_b_index and positive < batch_size / 2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True])\n",
    "            positive += 1\n",
    "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size / 2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False])\n",
    "            negative += 1\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c89d38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch = make_batch()\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5170add4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 256]),\n",
       " torch.Size([6, 256]),\n",
       " torch.Size([6, 20]),\n",
       " torch.Size([6, 20]),\n",
       " tensor([0, 0, 0, 1, 1, 1]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape, segment_ids.shape, masked_tokens.shape, masked_pos.shape, isNext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8987a739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 73112, 163717, 146919,  22505,  22505,   9744,  62405,  56467,  70665,\n",
       "          22505,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0],\n",
       "        [159046, 152056, 193488,  88897, 184702,  79067,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0],\n",
       "        [107932, 151712,  23486,  90423, 160793,  22505,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0],\n",
       "        [160793, 101181, 137789,  55481,  82588,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0],\n",
       "        [ 97007,  34624,  46710,  13199,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0],\n",
       "        [ 64604,   5909, 161217,  65454, 106745, 130672, 193121,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913698d0",
   "metadata": {},
   "source": [
    "### 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b73b42a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, max_len, n_segments, d_model, device):\n",
    "        super().__init__()\n",
    "        self.tok_embed = nn.Embedding(vocab_size, d_model)   # token embedding\n",
    "        self.pos_embed = nn.Embedding(max_len, d_model)      # position embedding\n",
    "        self.seg_embed = nn.Embedding(n_segments, d_model)   # segment embedding\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        # x, seg: (batch_size, seq_len)\n",
    "        seq_len = x.size(1)\n",
    "\n",
    "        # put pos on same device as x\n",
    "        pos = torch.arange(seq_len, dtype=torch.long, device=x.device)\n",
    "        pos = pos.unsqueeze(0).expand_as(x)  # (seq_len,) -> (batch_size, seq_len)\n",
    "\n",
    "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
    "        return self.norm(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9d6c3f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attn_pad_mask(seq_q, seq_k, device):\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    # eq(zero) is PAD token\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1).to(device)  # batch_size x 1 x len_k(=len_q), one is masking\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f0be8d",
   "metadata": {},
   "source": [
    "### Testing the attention mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1fb8b646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "print(get_attn_pad_mask(input_ids, input_ids, device).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835e6f23",
   "metadata": {},
   "source": [
    "### 4.3 Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f8bb0d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, d_ff, d_k, device):\n",
    "        super().__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention(n_heads, d_model, d_k, device)\n",
    "        self.pos_ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask)\n",
    "        enc_outputs = self.pos_ffn(enc_outputs)\n",
    "        return enc_outputs, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1cb344a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_k, device):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([d_k])).to(device)\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / self.scale # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context, attn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "acee1d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 6    # number of Encoder of Encoder Layer\n",
    "n_heads  = 8    # number of heads in Multi-Head Attention\n",
    "d_model  = 768  # Embedding Size\n",
    "d_ff = 768 * 4  # 4*d_model, FeedForward dimension\n",
    "d_k = d_v = 64  # dimension of K(=Q), V\n",
    "n_segments = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b9904cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, d_k, device):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_k\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_V = nn.Linear(d_model, self.d_v * n_heads)\n",
    "        self.device = device\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n",
    "        residual, batch_size = Q, Q.size(0)\n",
    "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
    "        k_s = self.W_K(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
    "        v_s = self.W_V(V).view(batch_size, -1, self.n_heads, self.d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k]\n",
    "\n",
    "        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        context, attn = ScaledDotProductAttention(self.d_k, self.device)(q_s, k_s, v_s, attn_mask)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_v) # context: [batch_size x len_q x n_heads * d_v]\n",
    "        output = nn.Linear(self.n_heads * self.d_v, self.d_model, device=self.device)(context)\n",
    "        return nn.LayerNorm(self.d_model, device=self.device)(output + residual), attn # output: [batch_size x len_q x d_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "754f8525",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch_size, len_seq, d_model) -> (batch_size, len_seq, d_ff) -> (batch_size, len_seq, d_model)\n",
    "        return self.fc2(F.gelu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf8a66c",
   "metadata": {},
   "source": [
    "### 4.4 Putting them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7c27cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self, n_layers, n_heads, d_model, d_ff, d_k, n_segments, vocab_size, max_len, device):\n",
    "        super(BERT, self).__init__()\n",
    "        self.params = {'n_layers': n_layers, 'n_heads': n_heads, 'd_model': d_model,\n",
    "                       'd_ff': d_ff, 'd_k': d_k, 'n_segments': n_segments,\n",
    "                       'vocab_size': vocab_size, 'max_len': max_len}\n",
    "        self.embedding = Embedding(vocab_size, max_len, n_segments, d_model, device)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(n_heads, d_model, d_ff, d_k, device) for _ in range(n_layers)])\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        self.activ = nn.Tanh()\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.classifier = nn.Linear(d_model, 2)\n",
    "        # decoder is shared with embedding layer\n",
    "        embed_weight = self.embedding.tok_embed.weight\n",
    "        n_vocab, n_dim = embed_weight.size()\n",
    "        self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n",
    "        self.decoder.weight = embed_weight\n",
    "        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, input_ids, segment_ids, masked_pos):\n",
    "        output = self.embedding(input_ids, segment_ids)\n",
    "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids, self.device)\n",
    "        for layer in self.layers:\n",
    "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
    "        # output : [batch_size, len, d_model], attn : [batch_size, n_heads, d_mode, d_model]\n",
    "        \n",
    "        # 1. predict next sentence\n",
    "        # it will be decided by first token(CLS)\n",
    "        h_pooled   = self.activ(self.fc(output[:, 0])) # [batch_size, d_model]\n",
    "        logits_nsp = self.classifier(h_pooled) # [batch_size, 2]\n",
    "\n",
    "        # 2. predict the masked token\n",
    "        masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1)) # [batch_size, max_pred, d_model]\n",
    "        h_masked = torch.gather(output, 1, masked_pos) # masking position [batch_size, max_pred, d_model]\n",
    "        h_masked  = self.norm(F.gelu(self.linear(h_masked)))\n",
    "        logits_lm = self.decoder(h_masked) + self.decoder_bias # [batch_size, max_pred, n_vocab]\n",
    "\n",
    "        return logits_lm, logits_nsp\n",
    "    \n",
    "    def get_last_hidden_state(self, input_ids, segment_ids):\n",
    "        output = self.embedding(input_ids, segment_ids)\n",
    "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids, self.device)\n",
    "        for layer in self.layers:\n",
    "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b7a671",
   "metadata": {},
   "source": [
    "### 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b26d0c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "n_layers = 12    # number of Encoder of Encoder Layer\n",
    "n_heads  = 12    # number of heads in Multi-Head Attention\n",
    "d_model  = 768  # Embedding Size\n",
    "d_ff = d_model * 4  # 4*d_model, FeedForward dimension\n",
    "d_k = d_v = 64  # dimension of K(=Q), V\n",
    "n_segments = 2\n",
    "\n",
    "num_epoch = 700\n",
    "model = BERT(n_layers, n_heads, d_model, d_ff, d_k, n_segments, vocab_size, max_len, device).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e26a1a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   0%|          | 0/700 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 loss = 168.357971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  14%|█▍        | 100/700 [06:04<37:23,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 loss = 2.679922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  29%|██▊       | 200/700 [13:16<33:20,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200 loss = 2.827845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  43%|████▎     | 300/700 [21:26<32:25,  4.86s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300 loss = 2.549943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  57%|█████▋    | 400/700 [30:43<26:55,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 400 loss = 2.789822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  71%|███████▏  | 500/700 [37:46<10:50,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 500 loss = 2.321024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  86%|████████▌ | 600/700 [43:58<06:02,  3.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 600 loss = 2.437893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 700/700 [50:06<00:00,  4.29s/it]\n"
     ]
    }
   ],
   "source": [
    "batch = make_batch()\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
    "\n",
    "# Move inputs to GPU\n",
    "input_ids = input_ids.to(device)\n",
    "segment_ids = segment_ids.to(device)\n",
    "masked_tokens = masked_tokens.to(device)\n",
    "masked_pos = masked_pos.to(device)\n",
    "isNext = isNext.to(device)\n",
    "\n",
    "# Wrap the epoch loop with tqdm\n",
    "for epoch in tqdm(range(num_epoch), desc=\"Training Epochs\"):\n",
    "    optimizer.zero_grad()\n",
    "    logits_lm, logits_nsp = model(input_ids, segment_ids, masked_pos)    \n",
    "    #logits_lm: (bs, max_mask, vocab_size) ==> (6, 5, 34)\n",
    "    #logits_nsp: (bs, yes/no) ==> (6, 2)\n",
    "\n",
    "    #1. mlm loss\n",
    "    #logits_lm.transpose: (bs, vocab_size, max_mask) vs. masked_tokens: (bs, max_mask)\n",
    "    loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens) # for masked LM\n",
    "    loss_lm = (loss_lm.float()).mean()\n",
    "    #2. nsp loss\n",
    "    #logits_nsp: (bs, 2) vs. isNext: (bs, )\n",
    "    loss_nsp = criterion(logits_nsp, isNext) # for sentence classification\n",
    "    \n",
    "    #3. combine loss\n",
    "    loss = loss_lm + loss_nsp\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch:', '%02d' % (epoch), 'loss =', '{:.6f}'.format(loss))\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "50e96a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to model_bert.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the model after training\n",
    "torch.save([model.params, model.state_dict()], 'model/model_bert.pth')\n",
    "print(\"Model saved to model_bert.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f507acad",
   "metadata": {},
   "source": [
    "5. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6180b8df",
   "metadata": {},
   "source": [
    "Since our dataset is very small, it won't work very well, but just for the sake of demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "370df068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model and all its hyperparameters\n",
    "params, state = torch.load('model/model_bert.pth')\n",
    "model_bert = BERT(**params, device=device).to(device)\n",
    "model_bert.load_state_dict(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "edee668f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'virtually', '[MASK]', 'major', 'police', '[MASK]', 'in', 'america—and', 'many', 'minor', 'ones—have', 'explosive', 'ordnance', 'disposal', 'robots', 'similar', 'to', 'the', '[MASK]', 'used', '[MASK]', 'dallas', '[SEP]', 'she', 'knows', 'that', 'diarrhoea', '[MASK]', 'caused', 'largely', 'by', 'people', 'ingesting', 'water', 'or', 'food', 'contaminated', 'by', 'human', 'waste', '–', 'kills', 'more', 'children', '[MASK]', 'than', 'hiv/aids', 'tuberculosis', 'and', 'malaria', 'combined', '[SEP]']\n",
      "masked tokens (words) :  ['every', 'tuberculosis', 'one', 'department', 'in', '–', 'robots', 'worldwide', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "masked tokens list :  [193814, 81530, 23486, 172424, 22934, 155412, 186001, 89035, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "masked tokens (words) :  ['[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "predict masked tokens list :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "1\n",
      "isNext :  False\n",
      "predict isNext :  True\n"
     ]
    }
   ],
   "source": [
    "# Predict mask tokens ans isNext\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(batch[1]))\n",
    "print([id2word[w.item()] for w in input_ids[0] if id2word[w.item()] != '[PAD]'])\n",
    "input_ids = input_ids.to(device)\n",
    "segment_ids = segment_ids.to(device)\n",
    "masked_tokens = masked_tokens.to(device)\n",
    "masked_pos = masked_pos.to(device)\n",
    "isNext = isNext.to(device)\n",
    "\n",
    "logits_lm, logits_nsp = model(input_ids, segment_ids, masked_pos)\n",
    "#logits_lm:  (1, max_mask, vocab_size) ==> (1, 5, 34)\n",
    "#logits_nsp: (1, yes/no) ==> (1, 2)\n",
    "\n",
    "#predict masked tokens\n",
    "#max the probability along the vocab dim (2), [1] is the indices of the max, and [0] is the first value\n",
    "logits_lm = logits_lm.data.cpu().max(2)[1][0].data.numpy() \n",
    "#note that zero is padding we add to the masked_tokens\n",
    "print('masked tokens (words) : ',[id2word[pos.item()] for pos in masked_tokens[0]])\n",
    "print('masked tokens list : ',[pos.item() for pos in masked_tokens[0]])\n",
    "print('masked tokens (words) : ',[id2word[pos.item()] for pos in logits_lm])\n",
    "print('predict masked tokens list : ', [pos for pos in logits_lm])\n",
    "\n",
    "#predict nsp\n",
    "logits_nsp = logits_nsp.cpu().data.max(1)[1][0].data.numpy()\n",
    "print(logits_nsp)\n",
    "print('isNext : ', True if isNext else False)\n",
    "print('predict isNext : ',True if logits_nsp else False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac5aa45",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bce993",
   "metadata": {},
   "source": [
    "# [Sentence-BERT](https://arxiv.org/pdf/1908.10084.pdf)\n",
    "\n",
    "[Reference Code](https://www.pinecone.io/learn/series/nlp/train-sentence-transformers-softmax/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29e7e356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import re\n",
    "from   random import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d086907a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/anushkaojha/2nd/NLP/NLP_Assignments/A4/.venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43df3e20",
   "metadata": {},
   "source": [
    "## 1. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb87aef",
   "metadata": {},
   "source": [
    "### Train, Test, Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50342a25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'premise': Value('string'),\n",
       "  'hypothesis': Value('string'),\n",
       "  'label': ClassLabel(names=['entailment', 'neutral', 'contradiction']),\n",
       "  'idx': Value('int32')},\n",
       " {'premise': Value('string'),\n",
       "  'hypothesis': Value('string'),\n",
       "  'label': ClassLabel(names=['entailment', 'neutral', 'contradiction'])})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "snli = datasets.load_dataset('snli')\n",
    "mnli = datasets.load_dataset('glue', 'mnli')\n",
    "mnli['train'].features, snli['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ead447d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'validation_matched', 'validation_mismatched', 'test_matched', 'test_mismatched'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of datasets to remove 'idx' column from\n",
    "mnli.column_names.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae321a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'idx' column from each dataset\n",
    "for column_names in mnli.column_names.keys():\n",
    "    mnli[column_names] = mnli[column_names].remove_columns('idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bace4df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'validation_matched', 'validation_mismatched', 'test_matched', 'test_mismatched'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnli.column_names.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fedfb00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2]), array([-1,  0,  1,  2]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.unique(mnli['train']['label']), np.unique(snli['train']['label'])\n",
    "#snli also have -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ad1feb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are -1 values in the label feature, these are where no class could be decided so we remove\n",
    "snli = snli.filter(\n",
    "    lambda x: 0 if x['label'] == -1 else 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fccdb16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2]), array([0, 1, 2]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.unique(mnli['train']['label']), np.unique(snli['train']['label'])\n",
    "#snli also have -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "55353fd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming you have your two DatasetDict objects named snli and mnli\n",
    "from datasets import DatasetDict\n",
    "# Merge the two DatasetDict objects\n",
    "raw_dataset = DatasetDict({\n",
    "    'train': datasets.concatenate_datasets([snli['train'], mnli['train']]).shuffle(seed=55).select(list(range(5000))),\n",
    "    'test': datasets.concatenate_datasets([snli['test'], mnli['test_mismatched']]).shuffle(seed=55).select(list(range(100))),\n",
    "    'validation': datasets.concatenate_datasets([snli['validation'], mnli['validation_mismatched']]).shuffle(seed=55).select(list(range(100)))\n",
    "})\n",
    "#remove .select(list(range(1000))) in order to use full dataset\n",
    "# Now, merged_dataset_dict contains the combined datasets from snli and mnli\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f304da58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executable: /Users/anushkaojha/2nd/NLP/NLP_Assignments/A4/.venv/bin/python\n",
      "Version: 3.9.6 (default, Dec  2 2025, 07:27:58) \n",
      "[Clang 17.0.0 (clang-1700.6.3.2)]\n",
      "User site: /Users/anushkaojha/Library/Python/3.9/lib/python/site-packages\n",
      "Site packages: ['/Users/anushkaojha/2nd/NLP/NLP_Assignments/A4/.venv/lib/python3.9/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys, site\n",
    "print(\"Executable:\", sys.executable)\n",
    "print(\"Version:\", sys.version)\n",
    "print(\"User site:\", site.getusersitepackages())\n",
    "print(\"Site packages:\", site.getsitepackages())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e61e8fe",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b0e5da19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/anushkaojha/2nd/NLP/NLP_Assignments/A4/.venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0672bc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "394a6f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5000/5000 [00:02<00:00, 2184.79 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 2194.42 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    max_seq_length = 128\n",
    "    padding = 'max_length'\n",
    "    # Tokenize the premise\n",
    "    premise_result = tokenizer(\n",
    "        examples['premise'], padding=padding, max_length=max_seq_length, truncation=True)\n",
    "    #num_rows, max_seq_length\n",
    "    # Tokenize the hypothesis\n",
    "    hypothesis_result = tokenizer(\n",
    "        examples['hypothesis'], padding=padding, max_length=max_seq_length, truncation=True)\n",
    "    #num_rows, max_seq_length\n",
    "    # Extract labels\n",
    "    labels = examples[\"label\"]\n",
    "    #num_rows\n",
    "    return {\n",
    "        \"premise_input_ids\": premise_result[\"input_ids\"],\n",
    "        \"premise_attention_mask\": premise_result[\"attention_mask\"],\n",
    "        \"hypothesis_input_ids\": hypothesis_result[\"input_ids\"],\n",
    "        \"hypothesis_attention_mask\": hypothesis_result[\"attention_mask\"],\n",
    "        \"labels\" : labels\n",
    "    }\n",
    "\n",
    "tokenized_datasets = raw_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['premise','hypothesis','label'])\n",
    "tokenized_datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "03bfa705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['premise_input_ids', 'premise_attention_mask', 'hypothesis_input_ids', 'hypothesis_attention_mask', 'labels'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['premise_input_ids', 'premise_attention_mask', 'hypothesis_input_ids', 'hypothesis_attention_mask', 'labels'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['premise_input_ids', 'premise_attention_mask', 'hypothesis_input_ids', 'hypothesis_attention_mask', 'labels'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e59cfdd",
   "metadata": {},
   "source": [
    "## 3. Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "29cf2266",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# initialize the dataloader\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets['train'], \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets['validation'], \n",
    "    batch_size=batch_size\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    tokenized_datasets['test'], \n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1c15116f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(batch['premise_input_ids'].shape)\n",
    "    print(batch['premise_attention_mask'].shape)\n",
    "    print(batch['hypothesis_input_ids'].shape)\n",
    "    print(batch['hypothesis_attention_mask'].shape)\n",
    "    print(batch['labels'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f258dfe3",
   "metadata": {},
   "source": [
    "## 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7db93339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# !{sys.executable} -m pip install --upgrade pip\n",
    "# !{sys.executable} -m pip install \\\n",
    "#     bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1f2dd64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert import *\n",
    "from bert_class import BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "26c4dcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the model trained and saved in task1\n",
    "load_path = 'model/model_bert.pth'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8c7f8ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params, state = torch.load(load_path, map_location= device)\n",
    "model = BERT(**params, device=device).to(device)\n",
    "model.load_state_dict(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223e672e",
   "metadata": {},
   "source": [
    "### Pooling\n",
    "SBERT adds a pooling operation to the output of BERT / RoBERTa to derive a fixed sized sentence embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "020183f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define mean pooling function\n",
    "def mean_pool(token_embeds, attention_mask):\n",
    "    # reshape attention_mask to cover 768-dimension embeddings\n",
    "    in_mask = attention_mask.unsqueeze(-1).expand(\n",
    "        token_embeds.size()\n",
    "    ).float()\n",
    "    # perform mean-pooling but exclude padding tokens (specified by in_mask)\n",
    "    pool = torch.sum(token_embeds * in_mask, 1) / torch.clamp(\n",
    "        in_mask.sum(1), min=1e-9\n",
    "    )\n",
    "    return pool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b4e60a",
   "metadata": {},
   "source": [
    "## 5. Loss Function\n",
    "\n",
    "## Classification Objective Function \n",
    "We concatenate the sentence embeddings $u$ and $v$ with the element-wise difference  $\\lvert u - v \\rvert $ and multiply the result with the trainable weight  $ W_t ∈  \\mathbb{R}^{3n \\times k}  $:\n",
    "\n",
    "$ o = \\text{softmax}\\left(W^T \\cdot \\left(u, v, \\lvert u - v \\rvert\\right)\\right) $\n",
    "\n",
    "where $n$ is the dimension of the sentence embeddings and k the number of labels. We optimize cross-entropy loss. This structure is depicted in Figure 1.\n",
    "\n",
    "## Regression Objective Function. \n",
    "The cosine similarity between the two sentence embeddings $u$ and $v$ is computed (Figure 2). We use means quared-error loss as the objective function.\n",
    "\n",
    "(Manhatten / Euclidean distance, semantically  similar sentences can be found.)\n",
    "\n",
    "<img src=\"./figures/sbert-architecture.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4f9df7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configurations(u,v):\n",
    "    # build the |u-v| tensor\n",
    "    uv = torch.sub(u, v)   # batch_size,hidden_dim\n",
    "    uv_abs = torch.abs(uv) # batch_size,hidden_dim\n",
    "    \n",
    "    # concatenate u, v, |u-v|\n",
    "    x = torch.cat([u, v, uv_abs], dim=-1) # batch_size, 3*hidden_dim\n",
    "    return x\n",
    "\n",
    "def cosine_similarity(u, v):\n",
    "    dot_product = np.dot(u, v)\n",
    "    norm_u = np.linalg.norm(u)\n",
    "    norm_v = np.linalg.norm(v)\n",
    "    similarity = dot_product / (norm_u * norm_v)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "37212455",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_head = torch.nn.Linear(768*3, 3).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "optimizer_classifier = torch.optim.Adam(classifier_head.parameters(), lr=2e-5)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e8d06ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# and setup a warmup for the first ~10% steps\n",
    "total_steps = int(len(raw_dataset) / batch_size)\n",
    "warmup_steps = int(0.1 * total_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "\t\toptimizer, num_warmup_steps=warmup_steps,\n",
    "  \tnum_training_steps=total_steps - warmup_steps\n",
    ")\n",
    "\n",
    "# then during the training loop we update the scheduler per step\n",
    "scheduler.step()\n",
    "\n",
    "scheduler_classifier = get_linear_schedule_with_warmup(\n",
    "\t\toptimizer_classifier, num_warmup_steps=warmup_steps,\n",
    "  \tnum_training_steps=total_steps - warmup_steps\n",
    ")\n",
    "\n",
    "# then during the training loop we update the scheduler per step\n",
    "scheduler_classifier.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947c5b63",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0edc76e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bad870c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [35:25<00:00,  6.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | loss = 1.208837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [26:57<00:00,  5.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | loss = 1.081702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [23:45<00:00,  4.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | loss = 1.397938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [21:57<00:00,  4.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | loss = 1.181835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [21:13<00:00,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | loss = 1.121003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "num_epoch = 5\n",
    "# 1 epoch should be enough, increase if wanted\n",
    "for epoch in range(num_epoch):\n",
    "    model.train()\n",
    "    classifier_head.train()\n",
    "    # initialize the dataloader loop with tqdm (tqdm == progress bar)\n",
    "    for step, batch in enumerate(tqdm(train_dataloader, leave=True)):\n",
    "        # zero all gradients on each new step\n",
    "        optimizer.zero_grad()\n",
    "        optimizer_classifier.zero_grad()\n",
    "\n",
    "        # prepare batches and more all to the active device\n",
    "        inputs_ids_a = batch['premise_input_ids'].to(device)\n",
    "        inputs_ids_b = batch['hypothesis_input_ids'].to(device)\n",
    "        attention_a = batch['premise_attention_mask'].to(device)\n",
    "        attention_b = batch['hypothesis_attention_mask'].to(device)\n",
    "\n",
    "        bs, seq_len = inputs_ids_a.shape\n",
    "        segment_ids = torch.zeros((bs, seq_len), dtype=torch.long, device=device)\n",
    "\n",
    "        label = batch['labels'].to(device)\n",
    "\n",
    "        # extract token embeddings from BERT at last_hidden_state\n",
    "        u_last_hidden_state = model.get_last_hidden_state(inputs_ids_a, segment_ids)\n",
    "        v_last_hidden_state = model.get_last_hidden_state(inputs_ids_b, segment_ids)\n",
    "\n",
    "        # get the mean pooled vectors\n",
    "        u_mean_pool = mean_pool(u_last_hidden_state, attention_a) # batch_size, hidden_dim\n",
    "        v_mean_pool = mean_pool(v_last_hidden_state, attention_b) # batch_size, hidden_dim\n",
    "\n",
    "        # build the |u-v| tensor\n",
    "        uv = torch.sub(u_mean_pool, v_mean_pool)   # batch_size,hidden_dim\n",
    "        uv_abs = torch.abs(uv) # batch_size,hidden_dim\n",
    "\n",
    "        # concatenate u, v, |u-v|\n",
    "        x = torch.cat([u_mean_pool, v_mean_pool, uv_abs], dim=-1) # batch_size, 3*hidden_dim\n",
    "\n",
    "        # process concatenated tensor through classifier_head\n",
    "        x = classifier_head(x) #batch_size, classifer\n",
    "\n",
    "        # calculate the 'softmax-loss' between predicted and true label\n",
    "        loss = criterion(x, label)\n",
    "\n",
    "        # using loss, calculate gradients and then optimizerize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer_classifier.step()\n",
    "\n",
    "        scheduler.step() # update learning rate scheduler\n",
    "        scheduler_classifier.step()\n",
    "\n",
    "    print(f'Epoch: {epoch + 1} | loss = {loss.item():.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3772e746",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "predictions = []\n",
    "probabilities = []\n",
    "classes = [\"entailment\", \"neutral\", \"contradiction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c912e948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Cosine Similarity: 0.9990\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "classifier_head.eval()\n",
    "total_similarity = 0\n",
    "with torch.no_grad():\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        # Move batches to the active device\n",
    "        inputs_ids_a = batch['premise_input_ids'].to(device)\n",
    "        inputs_ids_b = batch['hypothesis_input_ids'].to(device)\n",
    "        attention_a = batch['premise_attention_mask'].to(device)\n",
    "        attention_b = batch['hypothesis_attention_mask'].to(device)\n",
    "        segment_ids = torch.zeros(inputs_ids_a.shape[0], inputs_ids_a.shape[1], dtype=torch.int32).to(device)\n",
    "        label = batch['labels'].to(device)\n",
    "\n",
    "        # Extract token embeddings from BERT\n",
    "        u = model.get_last_hidden_state(inputs_ids_a, segment_ids)  # (batch_size, seq_len, hidden_dim)\n",
    "        v = model.get_last_hidden_state(inputs_ids_b, segment_ids)  # (batch_size, seq_len, hidden_dim)\n",
    "\n",
    "        # Get the mean pooled vectors (Keep them as Tensors)\n",
    "        u_mean_pool = mean_pool(u, attention_a)  # (batch_size, hidden_dim)\n",
    "        v_mean_pool = mean_pool(v, attention_b)  # (batch_size, hidden_dim)\n",
    "\n",
    "        # Computing cosine similarity\n",
    "        similarity_score = cosine_similarity(u_mean_pool.cpu().numpy().reshape(-1), v_mean_pool.cpu().numpy().reshape(-1))\n",
    "        total_similarity += similarity_score\n",
    "\n",
    "        # Concatenate [u, v, |u - v|]\n",
    "        uv_abs = torch.abs(u_mean_pool - v_mean_pool)  # [batch_size, hidden_dim]\n",
    "        x = torch.cat([u_mean_pool, v_mean_pool, uv_abs], dim=-1)  # [batch_size, 3*hidden_dim]\n",
    "\n",
    "        # Classification\n",
    "        logit_fn = classifier_head(x)  # (batch_size, num_classes)\n",
    "        probs = torch.nn.functional.softmax(logit_fn, dim=-1)\n",
    "\n",
    "        preds = torch.argmax(logit_fn, dim=-1)\n",
    "\n",
    "        labels.extend(label.cpu().tolist())\n",
    "        probabilities.extend(probs.cpu().tolist())\n",
    "        predictions.extend(preds.cpu().tolist())\n",
    "\n",
    "average_similarity = total_similarity / len(eval_dataloader)\n",
    "print(f\"Average Cosine Similarity: {average_similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4bc630c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "   entailment       0.00      0.00      0.00        62\n",
      "      neutral       0.44      0.10      0.16        72\n",
      "contradiction       0.33      0.92      0.49        66\n",
      "\n",
      "     accuracy                           0.34       200\n",
      "    macro avg       0.26      0.34      0.22       200\n",
      " weighted avg       0.27      0.34      0.22       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anushkaojha/2nd/NLP/NLP_Assignments/A4/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/anushkaojha/2nd/NLP/NLP_Assignments/A4/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/anushkaojha/2nd/NLP/NLP_Assignments/A4/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(labels, predictions, target_names=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5203b8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model\n",
    "torch.save({\n",
    "    \"bert_params\": model.params,\n",
    "    \"bert_state\": model.state_dict(),\n",
    "    \"clf_state\": classifier_head.state_dict(),\n",
    "    \"max_seq_length\": max_seq_length\n",
    "}, \"model/sen_bert_full.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9888c59",
   "metadata": {},
   "source": [
    "## 7. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a58598b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_similarity(model, tokenizer, sentence_a, sentence_b, device):\n",
    "    # Tokenize and convert sentences to input IDs and attention masks\n",
    "    inputs_a = tokenizer(sentence_a, return_tensors='pt', max_length=max_seq_length, truncation=True, padding='max_length').to(device)\n",
    "    inputs_b = tokenizer(sentence_b, return_tensors='pt', max_length=max_seq_length, truncation=True, padding='max_length').to(device)\n",
    "\n",
    "    # Move input IDs and attention masks to the active device\n",
    "    inputs_ids_a = inputs_a['input_ids']\n",
    "    attention_a = inputs_a['attention_mask']\n",
    "    inputs_ids_b = inputs_b['input_ids']\n",
    "    attention_b = inputs_b['attention_mask']\n",
    "    segment_ids = torch.zeros(1, max_seq_length, dtype=torch.int32).to(device)\n",
    "\n",
    "    # Extract token embeddings from BERT\n",
    "    u = model.get_last_hidden_state(inputs_ids_a, segment_ids)  # all token embeddings A = batch_size, seq_len, hidden_dim\n",
    "    v = model.get_last_hidden_state(inputs_ids_b, segment_ids)  # all token embeddings B = batch_size, seq_len, hidden_dim\n",
    "\n",
    "    # Get the mean-pooled vectors\n",
    "    u = mean_pool(u, attention_a).detach().cpu().numpy()  # shape (1, hidden_dim)\n",
    "    v = mean_pool(v, attention_b).detach().cpu().numpy()  # shape (1, hidden_dim)\n",
    "\n",
    "    similarity_score = cosine_similarity(u, v)[0, 0]      # scalar\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    return similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "087c91cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.9993\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "sentence_a = 'Your contribution helped make it possible for us to provide our students with a quality education.'\n",
    "sentence_b = \"Your contributions were of no help with our students' education.\"\n",
    "similarity = calculate_similarity(model, tokenizer, sentence_a, sentence_b, device)\n",
    "print(f\"Cosine Similarity: {similarity.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "41fd5939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.9993\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "sentence_a = ' A woman is cooking dinner in the kitchen.'\n",
    "sentence_b = 'A lady is preparing a meal at home.'\n",
    "similarity = calculate_similarity(model, tokenizer, sentence_a, sentence_b, device)\n",
    "print(f\"Cosine Similarity: {similarity.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63950cd",
   "metadata": {},
   "source": [
    "### Task: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "97b5e386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "58fcc884",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "pre_trained_model = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f23e3ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a3d7143f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sentence = [\"The cat is sleeping on the couch.\", \"The feline is resting on the sofa.\"]\n",
    "opp_sentence = [\"He is very punctual and reliable.\", \"You can never count on him to be on time.\"]\n",
    "encoded_input = tokenizer(pos_sentence, padding=True, truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7eceb34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model_output = pre_trained_model(**encoded_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b788ae51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.731993)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "sent_a_emb = sentence_embeddings[0].cpu().numpy().reshape(1, -1)\n",
    "sent_b_emb = sentence_embeddings[1].cpu().numpy().reshape(1, -1)\n",
    "cosine_similarity(sent_a_emb, sent_b_emb)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "480ca5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer(opp_sentence, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_output = pre_trained_model(**encoded_input)\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1160a723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.48303467)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_a_emb = sentence_embeddings[0].cpu().numpy().reshape(1, -1)\n",
    "sent_b_emb = sentence_embeddings[1].cpu().numpy().reshape(1, -1)\n",
    "cosine_similarity(sent_a_emb, sent_b_emb)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a859f61",
   "metadata": {},
   "source": [
    "## Classification Report\n",
    "\n",
    "The following table summarizes the performance of our Sentence-BERT model on the validation dataset. The evaluation metrics include Precision, Recall, F1-Score, and Support for each class.\n",
    "\n",
    "| Class           | Precision | Recall | F1-Score | Support |\n",
    "|----------------|-----------|--------|----------|---------|\n",
    "| Entailment     | 0.00      | 0.00   | 0.00     | 62      |\n",
    "| Neutral        | 0.44      | 0.10   | 0.16     | 72      |\n",
    "| Contradiction  | 0.33      | 0.92   | 0.49     | 66      |\n",
    "| **Accuracy**   |           |        | **0.34** | 200     |\n",
    "| **Macro Avg**  | 0.26      | 0.34   | 0.22     | 200     |\n",
    "| **Weighted Avg** | 0.27   | 0.34   | 0.22     | 200     |\n",
    "\n",
    "\n",
    "## Explanation for Zero Scores in Entailment\n",
    "\n",
    "The entailment class shows zero precision, recall, and F1-score because the model did not predict any samples as entailment during evaluation. When a class receives no predicted instances, precision becomes undefined and is automatically set to zero by the evaluation metric.\n",
    "\n",
    "This indicates that the classifier predominantly predicted the contradiction class. The high recall (0.92) for contradiction suggests that the model learned to favor this class, likely because it was easier to separate from the others using the learned sentence embeddings.\n",
    "\n",
    "Possible reasons for this behavior include:\n",
    "\n",
    "1. **Limited Training Data** – The model was trained on a relatively small subset of the dataset, reducing its ability to generalize across all classes.\n",
    "2. **Embedding Quality** – Since the base BERT model was trained from scratch on a limited corpus, the sentence embeddings may not capture nuanced semantic relationships required for entailment detection.\n",
    "3. **Class Prediction Bias** – During training, the classifier may have converged toward predicting the dominant or easiest class to minimize loss.\n",
    "\n",
    "Overall, the results indicate that while the model can strongly detect contradictions, it struggles to distinguish entailment and neutral relationships. This suggests that improved pretraining or larger training data would likely enhance performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab1d08c",
   "metadata": {},
   "source": [
    "## Comparison of Our Model with Pre-trained Model\n",
    "\n",
    "To evaluate semantic understanding, we compare our trained model against a pre-trained Sentence Transformer model using cosine similarity.\n",
    "\n",
    "| Model Type   | Cosine Similarity (Similar sentence) | Cosine Similarity (Dissimilar sentence) |\n",
    "|--------------|----------------------------------------|-------------------------------------------|\n",
    "| Our Model    | 0.993                                  | 0.993                                     |\n",
    "| Pre-trained  | 0.731                                  | 0.483                                     |\n",
    "\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "Both similar and dissimilar sentence pairs resulted in a cosine similarity score of **0.993**. This indicates that the model is not effectively distinguishing between semantically related and unrelated sentence pairs.\n",
    "\n",
    "Ideally, similar sentences should produce a high cosine similarity score, while dissimilar or contradictory sentences should yield a significantly lower score. Since both scores are nearly identical, this suggests that the sentence embeddings are not sufficiently discriminative.\n",
    "\n",
    "Possible reasons include:\n",
    "\n",
    "1. The base BERT model was trained from scratch on a limited dataset, leading to weak semantic representations.\n",
    "2. The classifier may have overfitted to a dominant class during training.\n",
    "3. The sentence embedding space may have collapsed, producing highly similar vectors regardless of input.\n",
    "\n",
    "These results highlight the importance of large-scale pretraining and proper fine-tuning when building robust sentence embedding models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7c1d7a",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "The implementation of BERT from scratch was carried out by taking reference from the professor’s provided materials. For pretraining, the Wikipedia dataset from Hugging Face was initially selected. However, due to hardware limitations, it was not feasible to train on the full dataset. As a result, the dataset was filtered down to 100,000 samples for training. After preprocessing and vocabulary construction, the BERT model class was implemented and training was initiated.\n",
    "\n",
    "During training, significant computational challenges were encountered. Memory constraints required reducing the batch size to 3 and limiting the number of epochs. Initially, the model was tested with a very large number of epochs (1000), but the loss showed minimal improvement and eventually the system ran out of memory. Consequently, the training configuration was adjusted to 700 epochs with reduced batch size. As expected, the limited dataset size and constrained training setup negatively affected model performance during inference.\n",
    "\n",
    "In Task 2, the SNLI and MNLI datasets were used to train a custom Sentence-BERT style model for Natural Language Inference. The implementation was again based on the professor’s reference code. After preprocessing and tokenization, the model was trained for 5 epochs. Similar to Task 1, memory limitations prevented the use of a larger batch size. The intended batch size of 32 exceeded available memory, so it was reduced to 8. While training completed successfully, the model struggled to generalize well across all classes.\n",
    "\n",
    "During evaluation and analysis (Task 3), the model’s performance was compared with a pre-trained model from Hugging Face. The comparison clearly demonstrated the performance gap between a model trained from scratch on limited data and a large-scale pre-trained transformer. The custom model showed weak discrimination across semantic classes, whereas the pre-trained model produced more stable and meaningful similarity representations.\n",
    "\n",
    "Overall, the main challenges encountered throughout this assignment were:\n",
    "\n",
    "- Limited model performance due to training from scratch  \n",
    "- Reduced dataset size caused by hardware constraints  \n",
    "- Memory limitations affecting batch size and training stability  \n",
    "- Computational resource restrictions preventing large-scale experimentation  \n",
    "\n",
    "### Proposed Improvements\n",
    "\n",
    "To improve the model’s performance in future work, the following strategies are suggested:\n",
    "\n",
    "- Increase the size of the training dataset  \n",
    "- Utilize more powerful hardware (GPU with larger memory)  \n",
    "- Experiment with larger batch sizes and optimized learning rates  \n",
    "- Increase model depth (more layers) and hidden dimensions  \n",
    "- Apply transfer learning using a pre-trained base model instead of training from scratch  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7233371c",
   "metadata": {},
   "source": [
    "## Web Application Interface Documentation\n",
    "\n",
    "For this assignment, I developed the web interface using Dash. The entire user interface along with the necessary model integration is implemented in the app.py file. It is a simple UI consisting of two text input fields, a Predict button, basic input validations, and a result display section. The demo of the application can be found in the README.md file inside the A4 folder.\n",
    "\n",
    "The model is integrated into the interface through a straightforward process. First, the trained model is loaded from the saved checkpoint (sen_bert_full.pth), and the stored weights are restored into the BERT model and classifier head. For tokenization, the BertTokenizer from bert-base-uncased is used. The input sentences are converted into embeddings using the get_last_hidden_state() method and mean pooling. Cosine similarity between the two sentence embeddings is computed, and the concatenated vector [u, v, |u - v|] is passed to the classifier to predict one of the three labels: Entailment, Neutral, or Contradiction. The prediction and similarity score are then displayed on the interface.\n",
    "\n",
    "The user interaction flow is as follows:\n",
    "- User enters two sentences\n",
    "- Sentences may express similar, neutral, or opposite meanings\n",
    "- User clicks the Predict button\n",
    "- The prediction and cosine similarity score are displayed on the screen\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (a4-venv)",
   "language": "python",
   "name": "a4-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
