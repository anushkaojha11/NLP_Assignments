{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd9a6aec",
   "metadata": {},
   "source": [
    "<h1>A2: Language Model<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cb02b1",
   "metadata": {},
   "source": [
    "<h2>Importing necessary libraries<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "110177c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext, datasets, math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "21c16c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fb906ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fb576a",
   "metadata": {},
   "source": [
    "<h2>Task 1. Dataset Acquisition<h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd4f766",
   "metadata": {},
   "source": [
    "The dataset used in this assignment is the Sherlock Holmes Collection obtained from Kaggle\n",
    "(https://www.kaggle.com/datasets/bharatkumar0925/sherlock-holmes-collection).\n",
    "It consists of the complete collection of public-domain novels and short stories written by Arthur Conan Doyle, including multiple Sherlock Holmes works. The dataset is provided in plain text format and represents a large continuous literary corpus suitable for language modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bd3b6c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines: 60365\n",
      "{'text': '                          THE COMPLETE SHERLOCK HOLMES'}\n"
     ]
    }
   ],
   "source": [
    "# Loading the raw text file containing the dataset\n",
    "with open(\"Sherlock Homes.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "\n",
    "# Splitting the raw text into individual non-empty lines\n",
    "# Each line is stored as a dictionary with a 'text' key\n",
    "\n",
    "data = [{\"text\": line} for line in text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "print(f\"Total lines: {len(data)}\")\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fdc538",
   "metadata": {},
   "source": [
    "<h1>Inspecting and cleaning the raw dataset<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87a6201",
   "metadata": {},
   "source": [
    "The raw text dataset was first loaded from a plain text file containing the complete Sherlock Holmes collection. To prepare the data for language modeling, several preprocessing steps needs to be applied. Non-narrative content such as front matter, chapter titles, section headers, and metadata needs to be removed using rule-based filtering. Empty lines are to be discarded, and the text is to be trimmed to retain only meaningful narrative content. After cleaning, the text will be tokenized using the basic_english tokenizer from TorchText, which lowercases text and separates punctuation. Special tokens for unknown words (<unk>) and end-of-sequence (<eos>) will be added. Finally, the tokenized text will be converted into numerical form using a vocabulary constructed from the corpus, enabling it to be used as input for training the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d44d0573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: \n",
      "2: \n",
      "3: \n",
      "4: \n",
      "5: THE COMPLETE SHERLOCK HOLMES\n",
      "6: \n",
      "7: Arthur Conan Doyle\n",
      "8: \n",
      "9: \n",
      "10: \n",
      "11: Table of contents\n",
      "12: \n",
      "13: A Study In Scarlet\n",
      "14: \n",
      "15: The Sign of the Four\n",
      "16: \n",
      "17: The Adventures of Sherlock Holmes\n",
      "18: A Scandal in Bohemia\n",
      "19: The Red-Headed League\n",
      "20: A Case of Identity\n",
      "21: The Boscombe Valley Mystery\n",
      "22: The Five Orange Pips\n",
      "23: The Man with the Twisted Lip\n",
      "24: The Adventure of the Blue Carbuncle\n",
      "25: The Adventure of the Speckled Band\n",
      "26: The Adventure of the Engineer's Thumb\n",
      "27: The Adventure of the Noble Bachelor\n",
      "28: The Adventure of the Beryl Coronet\n",
      "29: The Adventure of the Copper Beeches\n",
      "30: \n",
      "31: The Memoirs of Sherlock Holmes\n",
      "32: Silver Blaze\n",
      "33: The Yellow Face\n",
      "34: The Stock-Broker's Clerk\n",
      "35: The \"Gloria Scott\"\n",
      "36: The Musgrave Ritual\n",
      "37: The Reigate Squires\n",
      "38: The Crooked Man\n",
      "39: The Resident Patient\n",
      "40: The Greek Interpreter\n",
      "41: The Naval Treaty\n",
      "42: The Final Problem\n",
      "43: \n",
      "44: The Return of Sherlock Holmes\n",
      "45: The Adventure of the Empty House\n",
      "46: The Adventure of the Norwood Builder\n",
      "47: The Adventure of the Dancing Men\n",
      "48: The Adventure of the Solitary Cyclist\n",
      "49: The Adventure of the Priory School\n",
      "50: The Adventure of Black Peter\n",
      "51: The Adventure of Charles Augustus Milverton\n",
      "52: The Adventure of the Six Napoleons\n",
      "53: The Adventure of the Three Students\n",
      "54: The Adventure of the Golden Pince-Nez\n",
      "55: The Adventure of the Missing Three-Quarter\n",
      "56: The Adventure of the Abbey Grange\n",
      "57: The Adventure of the Second Stain\n",
      "58: \n",
      "59: The Hound of the Baskervilles\n",
      "60: \n",
      "61: The Valley Of Fear\n",
      "62: \n",
      "63: His Last Bow\n",
      "64: Preface\n",
      "65: The Adventure of Wisteria Lodge\n",
      "66: The Adventure of the Cardboard Box\n",
      "67: The Adventure of the Red Circle\n",
      "68: The Adventure of the Bruce-Partington Plans\n",
      "69: The Adventure of the Dying Detective\n",
      "70: The Disappearance of Lady Frances Carfax\n",
      "71: The Adventure of the Devil's Foot\n",
      "72: His Last Bow\n",
      "73: \n",
      "74: The Case-Book of Sherlock Holmes\n",
      "75: Preface\n",
      "76: The Illustrious Client\n",
      "77: The Blanched Soldier\n",
      "78: The Adventure Of The Mazarin Stone\n",
      "79: The Adventure of the Three Gables\n",
      "80: The Adventure of the Sussex Vampire\n",
      "81: The Adventure of the Three Garridebs\n",
      "82: The Problem of Thor Bridge\n",
      "83: The Adventure of the Creeping Man\n",
      "84: The Adventure of the Lion's Mane\n",
      "85: The Adventure of the Veiled Lodger\n",
      "86: The Adventure of Shoscombe Old Place\n",
      "87: The Adventure of the Retired Colourman\n",
      "88: \n",
      "89: \n",
      "90: \n",
      "91: \n",
      "92: \n",
      "93: \n",
      "94: \n",
      "95: \n",
      "96: \n",
      "97: \n",
      "98: \n",
      "99: A STUDY IN SCARLET\n",
      "100: \n",
      "101: \n",
      "102: \n",
      "103: \n",
      "104: \n",
      "105: Table of contents\n",
      "106: \n",
      "107: Part I\n",
      "108: Mr. Sherlock Holmes\n",
      "109: The Science Of Deduction\n",
      "110: The Lauriston Garden Mystery\n",
      "111: What John Rance Had To Tell\n",
      "112: Our Advertisement Brings A Visitor\n",
      "113: Tobias Gregson Shows What He Can Do\n",
      "114: Light In The Darkness\n",
      "115: \n",
      "116: Part II\n",
      "117: On The Great Alkali Plain\n",
      "118: The Flower Of Utah\n",
      "119: John Ferrier Talks With The Prophet\n",
      "120: A Flight For Life\n",
      "121: The Avenging Angels\n",
      "122: A Continuation Of The Reminiscences Of John Watson, M.D.\n",
      "123: The Conclusion\n",
      "124: \n",
      "125: \n",
      "126: \n",
      "127: \n",
      "128: \n",
      "129: \n",
      "130: \n",
      "131: \n",
      "132: \n",
      "133: \n",
      "134: \n",
      "135: \n",
      "136: \n",
      "137: \n",
      "138: \n",
      "139: \n",
      "140: \n",
      "141: \n",
      "142: \n",
      "143: PART I\n",
      "144: \n",
      "145: (Being a reprint from the reminiscences of\n",
      "146: John H. Watson, M.D.,\n",
      "147: late of the Army Medical Department.)\n",
      "148: \n",
      "149: \n",
      "150: \n",
      "151: \n",
      "152: \n",
      "153: CHAPTER I\n",
      "154: Mr. Sherlock Holmes\n",
      "155: \n",
      "156: \n",
      "157: In the year 1878 I took my degree of Doctor of Medicine of the\n",
      "158: University of London, and proceeded to Netley to go through the\n",
      "159: course prescribed for surgeons in the army. Having completed my\n",
      "160: studies there, I was duly attached to the Fifth Northumberland\n",
      "161: Fusiliers as Assistant Surgeon. The regiment was stationed in India\n",
      "162: at the time, and before I could join it, the second Afghan war had\n",
      "163: broken out. On landing at Bombay, I learned that my corps had\n",
      "164: advanced through the passes, and was already deep in the enemy's\n",
      "165: country. I followed, however, with many other officers who were in\n",
      "166: the same situation as myself, and succeeded in reaching Candahar in\n",
      "167: safety, where I found my regiment, and at once entered upon my new\n",
      "168: duties.\n",
      "169: \n",
      "170: The campaign brought honours and promotion to many, but for me it had\n",
      "171: nothing but misfortune and disaster. I was removed from my brigade\n",
      "172: and attached to the Berkshires, with whom I served at the fatal\n",
      "173: battle of Maiwand. There I was struck on the shoulder by a Jezail\n",
      "174: bullet, which shattered the bone and grazed the subclavian artery. I\n",
      "175: should have fallen into the hands of the murderous Ghazis had it not\n",
      "176: been for the devotion and courage shown by Murray, my orderly, who\n",
      "177: threw me across a pack-horse, and succeeded in bringing me safely to\n",
      "178: the British lines.\n",
      "179: \n",
      "180: Worn with pain, and weak from the prolonged hardships which I had\n",
      "181: undergone, I was removed, with a great train of wounded sufferers, to\n",
      "182: the base hospital at Peshawar. Here I rallied, and had already\n",
      "183: improved so far as to be able to walk about the wards, and even to\n",
      "184: bask a little upon the verandah, when I was struck down by enteric\n",
      "185: fever, that curse of our Indian possessions. For months my life was\n",
      "186: despaired of, and when at last I came to myself and became\n",
      "187: convalescent, I was so weak and emaciated that a medical board\n",
      "188: determined that not a day should be lost in sending me back to\n",
      "189: England. I was dispatched, accordingly, in the troopship Orontes, and\n",
      "190: landed a month later on Portsmouth jetty, with my health\n",
      "191: irretrievably ruined, but with permission from a paternal government\n",
      "192: to spend the next nine months in attempting to improve it.\n",
      "193: \n",
      "194: I had neither kith nor kin in England, and was therefore as free as\n",
      "195: air--or as free as an income of eleven shillings and sixpence a day\n",
      "196: will permit a man to be. Under such circumstances, I naturally\n",
      "197: gravitated to London, that great cesspool into which all the loungers\n",
      "198: and idlers of the Empire are irresistibly drained. There I stayed for\n",
      "199: some time at a private hotel in the Strand, leading a comfortless,\n",
      "200: meaningless existence, and spending such money as I had, considerably\n",
      "201: more freely than I ought. So alarming did the state of my finances\n",
      "202: become, that I soon realized that I must either leave the metropolis\n",
      "203: and rusticate somewhere in the country, or that I must make a\n",
      "204: complete alteration in my style of living. Choosing the latter\n",
      "205: alternative, I began by making up my mind to leave the hotel, and to\n",
      "206: take up my quarters in some less pretentious and less expensive\n",
      "207: domicile.\n",
      "208: \n",
      "209: On the very day that I had come to this conclusion, I was standing at\n",
      "210: the Criterion Bar, when some one tapped me on the shoulder, and\n",
      "211: turning round I recognized young Stamford, who had been a dresser\n",
      "212: under me at Bart's. The sight of a friendly face in the great\n",
      "213: wilderness of London is a pleasant thing indeed to a lonely man. In\n",
      "214: old days Stamford had never been a particular crony of mine, but now\n",
      "215: I hailed him with enthusiasm, and he, in his turn, appeared to be\n",
      "216: delighted to see me. In the exuberance of my joy, I asked him to\n",
      "217: lunch with me at the Holborn, and we started off together in a\n",
      "218: hansom.\n",
      "219: \n",
      "220: \"Whatever have you been doing with yourself, Watson?\" he asked in\n",
      "221: undisguised wonder, as we rattled through the crowded London streets.\n",
      "222: \"You are as thin as a lath and as brown as a nut.\"\n",
      "223: \n",
      "224: I gave him a short sketch of my adventures, and had hardly concluded\n",
      "225: it by the time that we reached our destination.\n",
      "226: \n",
      "227: \"Poor devil!\" he said, commiseratingly, after he had listened to my\n",
      "228: misfortunes. \"What are you up to now?\"\n",
      "229: \n",
      "230: \"Looking for lodgings,\" I answered. \"Trying to solve the problem as\n",
      "231: to whether it is possible to get comfortable rooms at a reasonable\n",
      "232: price.\"\n",
      "233: \n",
      "234: \"That's a strange thing,\" remarked my companion; \"you are the second\n",
      "235: man to-day that has used that expression to me.\"\n",
      "236: \n",
      "237: \"And who was the first?\" I asked.\n",
      "238: \n",
      "239: \"A fellow who is working at the chemical laboratory up at the\n",
      "240: hospital. He was bemoaning himself this morning because he could not\n",
      "241: get someone to go halves with him in some nice rooms which he had\n",
      "242: found, and which were too much for his purse.\"\n",
      "243: \n",
      "244: \"By Jove!\" I cried, \"if he really wants someone to share the rooms\n",
      "245: and the expense, I am the very man for him. I should prefer having a\n",
      "246: partner to being alone.\"\n",
      "247: \n",
      "248: Young Stamford looked rather strangely at me over his wine-glass.\n",
      "249: \"You don't know Sherlock Holmes yet,\" he said; \"perhaps you would not\n",
      "250: care for him as a constant companion.\"\n",
      "251: \n",
      "252: \"Why, what is there against him?\"\n",
      "253: \n",
      "254: \"Oh, I didn't say there was anything against him. He is a little\n",
      "255: queer in his ideas--an enthusiast in some branches of science. As far\n",
      "256: as I know he is a decent fellow enough.\"\n",
      "257: \n",
      "258: \"A medical student, I suppose?\" said I.\n",
      "259: \n",
      "260: \"No--I have no idea what he intends to go in for. I believe he is\n",
      "261: well up in anatomy, and he is a first-class chemist; but, as far as I\n",
      "262: know, he has never taken out any systematic medical classes. His\n",
      "263: studies are very desultory and eccentric, but he has amassed a lot of\n",
      "264: out-of-the way knowledge which would astonish his professors.\"\n",
      "265: \n",
      "266: \"Did you never ask him what he was going in for?\" I asked.\n",
      "267: \n",
      "268: \"No; he is not a man that it is easy to draw out, though he can be\n",
      "269: communicative enough when the fancy seizes him.\"\n",
      "270: \n",
      "271: \"I should like to meet him,\" I said. \"If I am to lodge with anyone, I\n",
      "272: should prefer a man of studious and quiet habits. I am not strong\n",
      "273: enough yet to stand much noise or excitement. I had enough of both in\n",
      "274: Afghanistan to last me for the remainder of my natural existence. How\n",
      "275: could I meet this friend of yours?\"\n",
      "276: \n",
      "277: \"He is sure to be at the laboratory,\" returned my companion. \"He\n",
      "278: either avoids the place for weeks, or else he works there from\n",
      "279: morning to night. If you like, we shall drive round together after\n",
      "280: luncheon.\"\n",
      "281: \n",
      "282: \"Certainly,\" I answered, and the conversation drifted away into other\n",
      "283: channels.\n",
      "284: \n",
      "285: As we made our way to the hospital after leaving the Holborn,\n",
      "286: Stamford gave me a few more particulars about the gentleman whom I\n",
      "287: proposed to take as a fellow-lodger.\n",
      "288: \n",
      "289: \"You mustn't blame me if you don't get on with him,\" he said; \"I know\n",
      "290: nothing more of him than I have learned from meeting him occasionally\n",
      "291: in the laboratory. You proposed this arrangement, so you must not\n",
      "292: hold me responsible.\"\n",
      "293: \n",
      "294: \"If we don't get on it will be easy to part company,\" I answered. \"It\n",
      "295: seems to me, Stamford,\" I added, looking hard at my companion, \"that\n",
      "296: you have some reason for washing your hands of the matter. Is this\n",
      "297: fellow's temper so formidable, or what is it? Don't be mealy-mouthed\n",
      "298: about it.\"\n",
      "299: \n",
      "300: \"It is not easy to express the inexpressible,\" he answered with a\n"
     ]
    }
   ],
   "source": [
    "#Inspecting few lines of dataset\n",
    "with open(\"Sherlock Homes.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in range(300):\n",
    "        print(f\"{i+1}: {f.readline().strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4b3fb3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cleaned lines: 60117\n",
      "First kept line: In the year 1878 I took my degree of Doctor of Medicine of the\n",
      "Last kept line: This text comes from the collection's version 3.1.\n"
     ]
    }
   ],
   "source": [
    "def clean_text_lines(file_path, start_phrase=\"In the year 1878\"):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Find the first line where the story begins\n",
    "    start_idx = None\n",
    "    for i, line in enumerate(lines):\n",
    "        if start_phrase in line:\n",
    "            start_idx = i\n",
    "            break\n",
    "\n",
    "    if start_idx is None:\n",
    "        raise ValueError(f\"Start phrase not found: {start_phrase}\")\n",
    "\n",
    "    cleaned_lines = []\n",
    "\n",
    "    # Keep everything from start_idx to end, with minimal cleaning\n",
    "    for line in lines[start_idx:]:\n",
    "        line = line.strip()\n",
    "\n",
    "        # Remove empty lines\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        # Remove ALL-CAPS headers like \"CHAPTER I\", \"PART II\", etc.\n",
    "        # (story content still remains)\n",
    "        if line.isupper():\n",
    "            continue\n",
    "\n",
    "        # Remove simple parenthetical metadata lines\n",
    "        if line.startswith(\"(\") and line.endswith(\")\"):\n",
    "            continue\n",
    "\n",
    "        cleaned_lines.append(line)\n",
    "\n",
    "    return cleaned_lines\n",
    "\n",
    "\n",
    "cleaned_data = clean_text_lines(\"Sherlock Homes.txt\", start_phrase=\"In the year 1878\")\n",
    "\n",
    "print(\"Total cleaned lines:\", len(cleaned_data))\n",
    "print(\"First kept line:\", cleaned_data[0])\n",
    "print(\"Last kept line:\", cleaned_data[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2803c55c",
   "metadata": {},
   "source": [
    "<h1>Tokenization<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "51667896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 762850\n",
      "['in', 'the', 'year', '1878', 'i', 'took', 'my', 'degree', 'of', 'doctor', 'of', 'medicine', 'of', 'the', 'university', 'of', 'london', ',', 'and', 'proceeded']\n"
     ]
    }
   ],
   "source": [
    "#After cleaning, the narrative text is tokenized using TorchTextâ€™s `basic_english` tokenizer.\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# Join all cleaned lines into ONE long text\n",
    "full_text = \" \".join(cleaned_data)\n",
    "\n",
    "# Tokenize once\n",
    "tokens = tokenizer(full_text)\n",
    "\n",
    "print(\"Total tokens:\", len(tokens))\n",
    "print(tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "45e5cba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned lines: 60117\n",
      "Total tokens: 762850\n"
     ]
    }
   ],
   "source": [
    "print(\"Cleaned lines:\", len(cleaned_data))\n",
    "print(\"Total tokens:\", sum(len(ex[\"tokens\"]) for ex in tokenized_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6a448f",
   "metadata": {},
   "source": [
    "<h1>Numericializing<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e21b7cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 12860\n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "vocab = build_vocab_from_iterator(\n",
    "    [tokens],   \n",
    "    min_freq=2\n",
    ")\n",
    "\n",
    "vocab.insert_token(\"<unk>\", 0)\n",
    "vocab.insert_token(\"<eos>\", 1)\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "print(\"Vocab size:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a9e3913e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('model/vocab_lm.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681fcfba",
   "metadata": {},
   "source": [
    "<h1>Prepare the batch loader<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72bb8d6",
   "metadata": {},
   "source": [
    "Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ecd1864f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset, vocab, batch_size):\n",
    "    data = []\n",
    "\n",
    "    # Iterating over each tokenized example (one line/sentence)\n",
    "    for example in dataset:\n",
    "        tokens = example[\"tokens\"] + [\"<eos>\"]   # FIX: proper EOS\n",
    "        tokens = [vocab[token] for token in tokens]\n",
    "        data.extend(tokens)\n",
    "\n",
    "    # Convert token list to a PyTorch tensor\n",
    "    data = torch.LongTensor(data)\n",
    "\n",
    "    num_batches = data.shape[0] // batch_size\n",
    "    data = data[:num_batches * batch_size]\n",
    "\n",
    "    # Reshape into [batch_size, sequence_length]\n",
    "    # Each row represents a continuous stream of tokens\n",
    "    data = data.view(batch_size, -1)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "03723014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 6429])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "# Convert tokenized text into a batched tensor of token indices\n",
    "# Output shape: [batch_size, sequence_length]\n",
    "train_data = get_data(tokenized_dataset, vocab, batch_size)\n",
    "\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8c3283",
   "metadata": {},
   "source": [
    "<h1>Task 2. Model Training<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de69ac2b",
   "metadata": {},
   "source": [
    "<h1>Modeling<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "54cdade7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hid_dim    = hid_dim\n",
    "        self.emb_dim    = emb_dim\n",
    "        \n",
    "        self.embedding  = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm       = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers, dropout=dropout_rate, batch_first=True)\n",
    "        self.dropout    = nn.Dropout(dropout_rate)\n",
    "        self.fc         = nn.Linear(hid_dim, vocab_size)\n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        init_range_emb = 0.1\n",
    "        init_range_other = 1/math.sqrt(self.hid_dim)\n",
    "        self.embedding.weight.data.uniform_(-init_range_emb, init_range_other)\n",
    "        self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n",
    "        self.fc.bias.data.zero_()\n",
    "        for i in range(self.num_layers):\n",
    "            self.lstm.all_weights[i][0] = torch.FloatTensor(self.emb_dim,\n",
    "                self.hid_dim).uniform_(-init_range_other, init_range_other) #We\n",
    "            self.lstm.all_weights[i][1] = torch.FloatTensor(self.hid_dim,   \n",
    "                self.hid_dim).uniform_(-init_range_other, init_range_other) #Wh\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        cell   = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        return hidden, cell\n",
    "        \n",
    "    def detach_hidden(self, hidden):\n",
    "        hidden, cell = hidden\n",
    "        hidden = hidden.detach() #not to be used for gradient computation\n",
    "        cell   = cell.detach()\n",
    "        return hidden, cell\n",
    "        \n",
    "    def forward(self, src, hidden):\n",
    "        #src: [batch_size, seq len]\n",
    "        embedding = self.dropout(self.embedding(src)) #harry potter is\n",
    "        #embedding: [batch-size, seq len, emb dim]\n",
    "        output, hidden = self.lstm(embedding, hidden)\n",
    "        #ouput: [batch size, seq len, hid dim]\n",
    "        #hidden: [num_layers * direction, seq len, hid_dim]\n",
    "        output = self.dropout(output)\n",
    "        prediction =self.fc(output)\n",
    "        #prediction: [batch_size, seq_len, vocab_size]\n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e143d00",
   "metadata": {},
   "source": [
    "The language model is implemented using a Long Short-Term Memory (LSTM) network, a type of recurrent neural network designed to capture long-range dependencies in sequential data. The LSTM contains memory cells and gating mechanisms that allow it to selectively remember or forget information from previous time steps. The input gate controls how much new information is written into the memory cell, the forget gate determines which information from the previous memory should be discarded or retained, and the output gate regulates how much information from the memory cell is exposed to the hidden state and output. This gating structure enables the model to effectively model long-term contextual relationships in text.\n",
    "\n",
    "In the implemented architecture, input tokens are first converted into dense vector representations using an embedding layer. These embeddings are then passed through stacked LSTM layers to capture temporal dependencies between words in the sequence. Dropout is applied to the embeddings and LSTM outputs to reduce overfitting by randomly deactivating neurons during training. Finally, a fully connected linear layer maps the LSTM hidden states to vocabulary-sized logits, which are used to predict the next word in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf5b34a",
   "metadata": {},
   "source": [
    "<h1>Training<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ecf8bb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 7,649,852 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary size\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Model hyperparameters\n",
    "emb_dim = 256\n",
    "hid_dim = 256\n",
    "num_layers = 2\n",
    "dropout_rate = 0.3\n",
    "lr = 1e-3\n",
    "\n",
    "# Initialize LSTM language model\n",
    "model = LSTMLanguageModel(\n",
    "    vocab_size, emb_dim, hid_dim, num_layers, dropout_rate\n",
    ").to(device)\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Number of trainable parameters\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"The model has {num_params:,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "14b89daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, seq_len, idx):\n",
    "    # Extract input sequence\n",
    "    src = data[:, idx:idx + seq_len]\n",
    "\n",
    "    # Target is the input shifted by one position\n",
    "    target = data[:, idx + 1:idx + seq_len + 1]\n",
    "\n",
    "    return src, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7b9f6948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    # Ensuring data length is divisible by seq_len\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches - 1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "\n",
    "    # Initializing hidden state at the start of each epoch\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc=\"Training\", leave=False):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Detaching hidden state to prevent backprop through entire history\n",
    "        hidden = model.detach_hidden(hidden)\n",
    "\n",
    "        # Getting input and target sequences\n",
    "        src, target = get_batch(data, seq_len, idx)\n",
    "        src, target = src.to(device), target.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        prediction, hidden = model(src, hidden)\n",
    "\n",
    "        # Reshape for loss computation\n",
    "        prediction = prediction.reshape(-1, prediction.size(-1))\n",
    "        target = target.reshape(-1)\n",
    "\n",
    "        # Compute loss and update parameters\n",
    "        loss = criterion(prediction, target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item() * seq_len\n",
    "\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4ad2aa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "\n",
    "    # Ensuring data length is divisible by seq_len\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches - 1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "\n",
    "    # Initializing hidden state\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, num_batches - 1, seq_len):\n",
    "            # Detach hidden state between batches\n",
    "            hidden = model.detach_hidden(hidden)\n",
    "\n",
    "            # Get input and target sequences\n",
    "            src, target = get_batch(data, seq_len, idx)\n",
    "            src, target = src.to(device), target.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            prediction, hidden = model(src, hidden)\n",
    "\n",
    "            # Reshape for loss computation\n",
    "            prediction = prediction.reshape(-1, prediction.size(-1))\n",
    "            target = target.reshape(-1)\n",
    "\n",
    "            loss = criterion(prediction, target)\n",
    "            epoch_loss += loss.item() * seq_len\n",
    "\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8f4934d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and validation (90% / 10%)\n",
    "split_idx = int(train_data.shape[1] * 0.9)\n",
    "\n",
    "train_data, valid_data = (\n",
    "    train_data[:, :split_idx],\n",
    "    train_data[:, split_idx:]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "60ea7179",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\tTrain Perplexity: 62.920\n",
      "\tValid Perplexity: 84.452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n",
      "\tTrain Perplexity: 61.962\n",
      "\tValid Perplexity: 83.858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50\n",
      "\tTrain Perplexity: 61.119\n",
      "\tValid Perplexity: 83.387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50\n",
      "\tTrain Perplexity: 60.233\n",
      "\tValid Perplexity: 83.388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50\n",
      "\tTrain Perplexity: 59.521\n",
      "\tValid Perplexity: 83.682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50\n",
      "\tTrain Perplexity: 58.012\n",
      "\tValid Perplexity: 83.176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50\n",
      "\tTrain Perplexity: 57.350\n",
      "\tValid Perplexity: 83.540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50\n",
      "\tTrain Perplexity: 56.802\n",
      "\tValid Perplexity: 83.096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50\n",
      "\tTrain Perplexity: 56.462\n",
      "\tValid Perplexity: 82.850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50\n",
      "\tTrain Perplexity: 55.964\n",
      "\tValid Perplexity: 82.738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50\n",
      "\tTrain Perplexity: 55.610\n",
      "\tValid Perplexity: 82.795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50\n",
      "\tTrain Perplexity: 55.466\n",
      "\tValid Perplexity: 82.549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50\n",
      "\tTrain Perplexity: 54.884\n",
      "\tValid Perplexity: 82.766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50\n",
      "\tTrain Perplexity: 54.385\n",
      "\tValid Perplexity: 82.879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50\n",
      "\tTrain Perplexity: 53.793\n",
      "\tValid Perplexity: 82.623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50\n",
      "\tTrain Perplexity: 53.480\n",
      "\tValid Perplexity: 82.691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50\n",
      "\tTrain Perplexity: 53.104\n",
      "\tValid Perplexity: 82.497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50\n",
      "\tTrain Perplexity: 52.988\n",
      "\tValid Perplexity: 82.489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50\n",
      "\tTrain Perplexity: 52.772\n",
      "\tValid Perplexity: 82.628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50\n",
      "\tTrain Perplexity: 52.616\n",
      "\tValid Perplexity: 82.489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50\n",
      "\tTrain Perplexity: 52.485\n",
      "\tValid Perplexity: 82.486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50\n",
      "\tTrain Perplexity: 52.330\n",
      "\tValid Perplexity: 82.475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50\n",
      "\tTrain Perplexity: 52.313\n",
      "\tValid Perplexity: 82.471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50\n",
      "\tTrain Perplexity: 52.287\n",
      "\tValid Perplexity: 82.487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50\n",
      "\tTrain Perplexity: 52.243\n",
      "\tValid Perplexity: 82.500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50\n",
      "\tTrain Perplexity: 52.163\n",
      "\tValid Perplexity: 82.481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50\n",
      "\tTrain Perplexity: 52.221\n",
      "\tValid Perplexity: 82.496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50\n",
      "\tTrain Perplexity: 52.181\n",
      "\tValid Perplexity: 82.483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50\n",
      "\tTrain Perplexity: 52.190\n",
      "\tValid Perplexity: 82.488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50\n",
      "\tTrain Perplexity: 52.133\n",
      "\tValid Perplexity: 82.487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50\n",
      "\tTrain Perplexity: 52.158\n",
      "\tValid Perplexity: 82.489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50\n",
      "\tTrain Perplexity: 52.095\n",
      "\tValid Perplexity: 82.492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50\n",
      "\tTrain Perplexity: 52.137\n",
      "\tValid Perplexity: 82.491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50\n",
      "\tTrain Perplexity: 52.094\n",
      "\tValid Perplexity: 82.492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50\n",
      "\tTrain Perplexity: 52.095\n",
      "\tValid Perplexity: 82.492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/50\n",
      "\tTrain Perplexity: 52.129\n",
      "\tValid Perplexity: 82.492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/50\n",
      "\tTrain Perplexity: 52.103\n",
      "\tValid Perplexity: 82.493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50\n",
      "\tTrain Perplexity: 52.148\n",
      "\tValid Perplexity: 82.493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50\n",
      "\tTrain Perplexity: 52.167\n",
      "\tValid Perplexity: 82.493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50\n",
      "\tTrain Perplexity: 52.129\n",
      "\tValid Perplexity: 82.493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/50\n",
      "\tTrain Perplexity: 52.125\n",
      "\tValid Perplexity: 82.493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/50\n",
      "\tTrain Perplexity: 52.120\n",
      "\tValid Perplexity: 82.493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50\n",
      "\tTrain Perplexity: 52.145\n",
      "\tValid Perplexity: 82.493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/50\n",
      "\tTrain Perplexity: 52.090\n",
      "\tValid Perplexity: 82.493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50\n",
      "\tTrain Perplexity: 52.153\n",
      "\tValid Perplexity: 82.493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50\n",
      "\tTrain Perplexity: 52.107\n",
      "\tValid Perplexity: 82.493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50\n",
      "\tTrain Perplexity: 52.094\n",
      "\tValid Perplexity: 82.493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50\n",
      "\tTrain Perplexity: 52.117\n",
      "\tValid Perplexity: 82.493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50\n",
      "\tTrain Perplexity: 52.163\n",
      "\tValid Perplexity: 82.493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50\n",
      "\tTrain Perplexity: 52.139\n",
      "\tValid Perplexity: 82.493\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch.optim as optim\n",
    "\n",
    "# Training configuration\n",
    "n_epochs = 50\n",
    "seq_len = 50\n",
    "clip = 0.25\n",
    "\n",
    "# Learning rate scheduler\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, factor=0.5, patience=1\n",
    ")\n",
    "\n",
    "best_valid_loss = float(\"inf\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(\n",
    "        model, train_data, optimizer, criterion,\n",
    "        batch_size, seq_len, clip, device\n",
    "    )\n",
    "\n",
    "    valid_loss = evaluate(\n",
    "        model, valid_data, criterion, batch_size,\n",
    "        seq_len, device\n",
    "    )\n",
    "\n",
    "    lr_scheduler.step(valid_loss)\n",
    "\n",
    "    # Save best model based on validation loss\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), \"model/best-val-lstm_lm.pt\")\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{n_epochs}\")\n",
    "    print(f\"\\tTrain Perplexity: {math.exp(train_loss):.3f}\")\n",
    "    print(f\"\\tValid Perplexity: {math.exp(valid_loss):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dd0ae0",
   "metadata": {},
   "source": [
    "Firstly, the model hyperparameters such as vocabulary size, embedding dimension, hidden dimension, number of LSTM layers, dropout rate, and learning rate are initialized. The Adam optimizer is used to optimize the model parameters, and the CrossEntropyLoss criterion is employed to compute the training loss.\n",
    "\n",
    "The model is trained for a fixed number of epochs. In each epoch, the training data is divided into fixed-length sequences using the get_batch function. At the beginning of each epoch, the hidden state of the LSTM is reset. For every batch, the optimizer gradients are cleared, a forward pass is performed, and the loss is calculated by comparing the predicted probability distribution of the next token with the actual next token. Gradients are computed using backpropagation, and the model parameters are updated using the optimizer. The training loss is accumulated across all batches within an epoch.\n",
    "\n",
    "After completing an epoch, the model is switched to evaluation mode, and the validation data is processed using the same batching procedure without updating model parameters. The validation loss is computed to assess model performance on unseen data. A learning rate scheduler adjusts the learning rate based on validation loss, and the model parameters are saved whenever the validation loss improves over previous epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0e9d7e",
   "metadata": {},
   "source": [
    "<h1>Testing<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "52c771e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Perplexity: 82.471\n"
     ]
    }
   ],
   "source": [
    "# Loading the  model \n",
    "model.load_state_dict(\n",
    "    torch.load(\"model/best-val-lstm_lm.pt\", map_location=device)\n",
    ")\n",
    "\n",
    "# Evaluating the model on the validation  data\n",
    "test_loss = evaluate(\n",
    "    model, valid_data, criterion, batch_size, seq_len, device\n",
    ")\n",
    "\n",
    "print(f\"Test Perplexity: {math.exp(test_loss):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264744bd",
   "metadata": {},
   "source": [
    "<h1>Real-world inference<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "1aa95533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=None):\n",
    "    # Setting random seed for reproducibility \n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenizing and numericalizing the input prompt\n",
    "    tokens = tokenizer(prompt)\n",
    "    if len(tokens) == 0:\n",
    "        tokens = [\"<unk>\"]\n",
    "    indices = [vocab[t] for t in tokens]\n",
    "\n",
    "    batch_size = 1\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in indices[:-1]:\n",
    "            src = torch.LongTensor([[idx]]).to(device)\n",
    "            _, hidden = model(src, hidden)\n",
    "\n",
    "        last_idx = indices[-1]\n",
    "\n",
    "        for _ in range(max_seq_len):\n",
    "            src = torch.LongTensor([[last_idx]]).to(device)\n",
    "            prediction, hidden = model(src, hidden)\n",
    "\n",
    "            logits = prediction[:, -1, :] / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "            next_idx = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            while next_idx == vocab[\"<unk>\"]:\n",
    "                next_idx = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            if next_idx == vocab[\"<eos>\"]:\n",
    "                break\n",
    "\n",
    "            indices.append(next_idx)\n",
    "            last_idx = next_idx\n",
    "\n",
    "    # Converting indices back to tokens\n",
    "    itos = vocab.get_itos()\n",
    "    return [itos[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "1b6c40df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: Mr. Sherlock Holmes \n",
      "\n",
      "temp=0.1:\n",
      "mr . sherlock holmes ,\n",
      "\n",
      "temp=0.5:\n",
      "mr . sherlock holmes , he\n",
      "\n",
      "temp=0.7:\n",
      "mr . sherlock holmes , he ' s , and there is a small\n",
      "\n",
      "temp=0.9:\n",
      "mr . sherlock holmes , he ' s beard , came at the gate\n",
      "\n",
      "temp=1.0:\n",
      "mr . sherlock holmes , he ' s beard , came after the\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example prompts\n",
    "prompts = [\n",
    "    \"Mr. Sherlock Holmes\"\n",
    "]\n",
    "\n",
    "max_seq_len = 100\n",
    "seed = 91\n",
    "\n",
    "# Smaller temperature -> less random (more confident); larger -> more diverse\n",
    "temperatures = [0.1, 0.5, 0.7, 0.9, 1.0]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(\"PROMPT:\", prompt, \"\\n\")\n",
    "    for temperature in temperatures:\n",
    "        generation = generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed)\n",
    "        print(f\"temp={temperature}:\\n\" + \" \".join(generation) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e12eea7",
   "metadata": {},
   "source": [
    "<h1>Documentation of Web Interface<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd6da77",
   "metadata": {},
   "source": [
    "The web application for text generation is implemented using the **Dash** framework. The complete user interface and model integration logic are contained within the `app.py` file. The interface is intentionally kept simple and consists of a text input field for user prompts, a **Generate** button, and an output section where the generated text is displayed for multiple temperature values. A demonstration of the application and usage instructions are provided in the `README.md` file inside the `A2` folder.\n",
    "\n",
    "\n",
    "### Model Integration with the Web Application\n",
    "\n",
    "The trained LSTM language model is integrated into the web application through the following steps:\n",
    "\n",
    "- The vocabulary generated during training is loaded using Pythonâ€™s `pickle` module.  \n",
    "- The LSTM language model architecture is recreated using the same hyperparameters as used during training.  \n",
    "- Pre-trained model weights are loaded into the model using `torch.load`.  \n",
    "- A text generation function (`generate_text`) is defined, which tokenizes user input, converts tokens into numerical indices using the vocabulary, and generates new text using the trained LSTM model.  \n",
    "- Temperature values are applied during inference to control the randomness and diversity of generated text.\n",
    "\n",
    "During inference, the model is run in evaluation mode. Predictions are sampled from the probability distribution produced by the model until either the maximum generation length is reached or an end-of-sequence token (`<eos>`) is encountered.\n",
    "\n",
    "\n",
    "\n",
    "### User Interaction Flow\n",
    "\n",
    "The interaction between the user, the web interface, and the language model follows these steps:\n",
    "\n",
    "- The user enters a text prompt into the input field (e.g., *â€œSherlock Holmes isâ€*).  \n",
    "- The user clicks the **Generate** button.  \n",
    "- The application passes the prompt to the language model.  \n",
    "- The model generates continuations of the prompt for multiple temperature values (0.1, 0.5, 0.7, 0.9, and 1.0).  \n",
    "- The generated text outputs are displayed on the web page, clearly labeled by temperature.\n",
    "\n",
    "Since the language model is trained exclusively on the **Sherlock Holmes literary corpus**, prompts related to similar themes, characters, or narrative styles produce more coherent and meaningful outputs. Inputs unrelated to this domain may result in less sensible or generic text, reflecting the domain-specific nature of the trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be02586",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv311 (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
