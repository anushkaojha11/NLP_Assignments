{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb3d917b",
   "metadata": {},
   "source": [
    "<h1>Importing necessary libraries<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ad7e834",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/anushkaojha/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"brown\")\n",
    "\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3308593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "#Setting up device \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f749e4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuring Reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4e09a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the \"news\" category from Brown corpus\n",
    "corpus = brown.sents(categories=\"news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c000cbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking a small subset for faster training.\n",
    "corpus = corpus[:500]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b452e808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 11711\n"
     ]
    }
   ],
   "source": [
    "#Flattening the corpus for vocabulary building\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "news_flatten = flatten(corpus)\n",
    "\n",
    "print(\"Number of unique tokens:\", len(news_flatten))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37b06859",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building vocabulary and adding <UNK>\n",
    "counts = Counter(news_flatten)\n",
    "vocab = sorted(counts.keys(), key=lambda w: (-counts[w], w))\n",
    "vocab.append(\"<UNK>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d344c48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size (including <UNK>): 2947\n"
     ]
    }
   ],
   "source": [
    "#Mapping the word and index \n",
    "word2index = {w: i for i, w in enumerate(vocab)}\n",
    "index2word = {i: w for w, i in word2index.items()}\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocab size (including <UNK>):\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb97136",
   "metadata": {},
   "source": [
    "Creating function to generate random training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d1b3859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(batch_size, corpus, window_size=2):\n",
    "\n",
    "    skip_grams = []\n",
    "    unk = word2index[\"<UNK>\"]\n",
    "\n",
    "\n",
    "    # Loop through each sentence in the corpus\n",
    "    for sent in corpus:\n",
    "\n",
    "        # We start from index = window_size and stop at len(sent) - window_size\n",
    "        # so that each target word has a full context window on both sides\n",
    "        for i in range(window_size, len(sent) - window_size):\n",
    "\n",
    "            # Center (target) word\n",
    "            target = word2index[sent[i]]\n",
    "\n",
    "            # Collect context words within the window\n",
    "            context = []\n",
    "            for w in range(1, window_size + 1):\n",
    "                # Left context\n",
    "                context.append(word2index[sent[i - w]])\n",
    "                # Right context\n",
    "                context.append(word2index[sent[i + w]])\n",
    "\n",
    "            # Create skip-gram pairs (target, context)\n",
    "            for ctx_word in context:\n",
    "                skip_grams.append([target, ctx_word])\n",
    "\n",
    "    # Randomly sample skip-gram pairs to form a mini-batch\n",
    "    random_inputs = []\n",
    "    random_labels = []\n",
    "\n",
    "    random_index = np.random.choice(\n",
    "        range(len(skip_grams)), batch_size, replace=False\n",
    "    )\n",
    "\n",
    "    for idx in random_index:\n",
    "        random_inputs.append([skip_grams[idx][0]])  # center word\n",
    "        random_labels.append([skip_grams[idx][1]])  # context word\n",
    "\n",
    "    return np.array(random_inputs), np.array(random_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f92bfe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch: [[   3]\n",
      " [1763]]\n",
      "Target batch: [[1]\n",
      " [7]]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "input_batch, target_batch = random_batch(batch_size, corpus, window_size=2)\n",
    "print(\"Input batch:\", input_batch)\n",
    "print(\"Target batch:\", target_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2f046d",
   "metadata": {},
   "source": [
    "<h1>Skipgram without negative sampling<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1465b7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skipgram(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(Skipgram, self).__init__()\n",
    "        self.embedding_center  = nn.Embedding(vocab_size, emb_size)  # v vectors\n",
    "        self.embedding_outside = nn.Embedding(vocab_size, emb_size)  # u vectors\n",
    "\n",
    "    def forward(self, center, outside, all_vocab):\n",
    "        \"\"\"\n",
    "        center:  [batch_size, 1]\n",
    "        outside: [batch_size, 1]\n",
    "        all_vocab: [batch_size, vocab_size] (same vocab list repeated across batch)\n",
    "        \"\"\"\n",
    "        # Get embeddings\n",
    "        v = self.embedding_center(center)          # [B, 1, D]\n",
    "        u_o = self.embedding_outside(outside)      # [B, 1, D]\n",
    "        u_all = self.embedding_outside(all_vocab)  # [B, V, D]\n",
    "\n",
    "        # Numerator score: u_o dot v\n",
    "        # [B,1,D] @ [B,D,1] -> [B,1,1] -> squeeze -> [B,1]\n",
    "        numerator = torch.exp(u_o.bmm(v.transpose(1, 2)).squeeze(2))  # [B,1]\n",
    "\n",
    "        # Denominator: sum_{w in vocab} exp(u_w dot v)\n",
    "        # [B,V,D] @ [B,D,1] -> [B,V,1] -> squeeze -> [B,V]\n",
    "        denom_scores = u_all.bmm(v.transpose(1, 2)).squeeze(2)        # [B,V]\n",
    "        denominator = torch.sum(torch.exp(denom_scores), dim=1, keepdim=True)  # [B,1]\n",
    "\n",
    "        # Negative log likelihood (scalar loss)\n",
    "        loss = -torch.mean(torch.log(numerator / denominator))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ffa0680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "import torch.optim as optim\n",
    "batch_size = 2\n",
    "embedding_size = 2\n",
    "window_size = 2\n",
    "model = Skipgram(vocab_size, embedding_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = [\n",
    "        word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"]\n",
    "        for w in seq\n",
    "    ]\n",
    "    return torch.LongTensor(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35be0a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_vocabs shape: torch.Size([2, 2947])\n"
     ]
    }
   ],
   "source": [
    "all_vocabs = prepare_sequence(list(vocab), word2index) \\\n",
    "                .expand(batch_size, len(vocab)) \\\n",
    "                .to(device)\n",
    "\n",
    "print(\"all_vocabs shape:\", all_vocabs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48150bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d30a81",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1f9fe40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000 | cost: 8.358099 | time: 12.14 ms\n",
      "Epoch: 2000 | cost: 7.793187 | time: 11.80 ms\n",
      "Epoch: 3000 | cost: 7.661410 | time: 62.18 ms\n",
      "Epoch: 4000 | cost: 6.302114 | time: 11.88 ms\n",
      "Epoch: 5000 | cost: 7.772816 | time: 64.14 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "num_epochs = 5000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Get a random mini-batch of Skip-gram pairs\n",
    "    # window_size is now configurable (Task 1 requirement)\n",
    "    input_batch, target_batch = random_batch(\n",
    "        batch_size, corpus, window_size=window_size\n",
    "    )\n",
    "\n",
    "    # Convert numpy arrays to torch tensors\n",
    "    input_batch  = torch.LongTensor(input_batch).to(device)   # [batch_size, 1]\n",
    "    target_batch = torch.LongTensor(target_batch).to(device)  # [batch_size, 1]\n",
    "\n",
    "    # Zero gradients from previous step\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass: compute Skip-gram loss\n",
    "    loss = model(input_batch, target_batch, all_vocabs)\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # Update model parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    # Compute elapsed time for this epoch\n",
    "    epoch_time_ms = (end - start) * 1000\n",
    "\n",
    "    # Print training progress every 1000 epochs (same as professor)\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(\n",
    "            f\"Epoch: {epoch + 1} | \"\n",
    "            f\"cost: {loss.item():.6f} | \"\n",
    "            f\"time: {epoch_time_ms:.2f} ms\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36fb8bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is', 'as', 'has', 'it', 'not', 'will', 'at', 'with', 'an', 'his']\n"
     ]
    }
   ],
   "source": [
    "print(vocab[20:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2707a5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine Similarity for Skip-gram embeddings\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "def get_embed_skip_gram(word):\n",
    "\n",
    "    idx = word2index.get(word, word2index[\"<UNK>\"])\n",
    "    idx_tensor = torch.LongTensor([idx]).to(device)\n",
    "\n",
    "    v_embed = model.embedding_center(idx_tensor)\n",
    "    u_embed = model.embedding_outside(idx_tensor)\n",
    "\n",
    "    word_embed = (v_embed + u_embed) / 2.0\n",
    "    return word_embed.squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    return dot(a, b) / (norm(a) * norm(b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03372107",
   "metadata": {},
   "outputs": [],
   "source": [
    "election = get_embed_skip_gram(\"election\")\n",
    "vote = get_embed_skip_gram(\"vote\")\n",
    "campaign = get_embed_skip_gram(\"campaign\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "569719a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "election vs vote:        0.4850\n",
      "election vs campaign:   -0.0720\n",
      "election vs election:   1.0000\n"
     ]
    }
   ],
   "source": [
    "print(f\"election vs vote:        {cos_sim(election, vote):.4f}\")\n",
    "print(f\"election vs campaign:   {cos_sim(election, campaign):.4f}\")\n",
    "print(f\"election vs election:   {cos_sim(election, election):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb485ac",
   "metadata": {},
   "source": [
    "saving the skipgram model without negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b19bf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "import pickle\n",
    "torch.save(model.state_dict(), 'model/skipgram_model.pth')\n",
    "pickle.dump(model, open('model/skipgram.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6db4b7e",
   "metadata": {},
   "source": [
    "<h1>Word2Vec (Negative Sampling)<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7aea231",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building unigram table\n",
    "Z = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c57c87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "word_count = Counter(flatten(corpus))\n",
    "num_total_words = sum(word_count.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d609c5b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11711"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b6f7208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram table size: 3623\n"
     ]
    }
   ],
   "source": [
    "unigram_table = []\n",
    "for w in vocab:\n",
    "    uw = word_count[w] / max(1, num_total_words)     # unigram prob\n",
    "    uw_alpha = int((uw ** 0.75) / Z)                 # apply 0.75 smoothing\n",
    "    if uw_alpha > 0:\n",
    "        unigram_table.extend([w] * uw_alpha)\n",
    "\n",
    "print(\"Unigram table size:\", len(unigram_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "191bfe5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'the': 120,\n",
       "         ',': 87,\n",
       "         '.': 85,\n",
       "         'of': 75,\n",
       "         'to': 65,\n",
       "         'a': 52,\n",
       "         'in': 49,\n",
       "         'and': 46,\n",
       "         '``': 33,\n",
       "         'for': 33,\n",
       "         'that': 33,\n",
       "         \"''\": 33,\n",
       "         'The': 28,\n",
       "         'said': 27,\n",
       "         'be': 25,\n",
       "         'would': 24,\n",
       "         'on': 24,\n",
       "         'was': 23,\n",
       "         'by': 22,\n",
       "         'he': 21,\n",
       "         'is': 21,\n",
       "         'as': 18,\n",
       "         'has': 17,\n",
       "         'it': 15,\n",
       "         'not': 15,\n",
       "         'will': 14,\n",
       "         'at': 14,\n",
       "         'with': 14,\n",
       "         'an': 13,\n",
       "         'his': 13,\n",
       "         'been': 13,\n",
       "         'which': 12,\n",
       "         'He': 12,\n",
       "         'this': 11,\n",
       "         '--': 11,\n",
       "         'Mr.': 11,\n",
       "         'more': 11,\n",
       "         'have': 10,\n",
       "         'who': 10,\n",
       "         'from': 10,\n",
       "         'President': 9,\n",
       "         'administration': 9,\n",
       "         'its': 9,\n",
       "         'year': 9,\n",
       "         'are': 9,\n",
       "         'had': 9,\n",
       "         'or': 9,\n",
       "         'State': 9,\n",
       "         'Texas': 9,\n",
       "         'election': 9,\n",
       "         'jury': 9,\n",
       "         'last': 8,\n",
       "         'there': 8,\n",
       "         'House': 8,\n",
       "         'It': 8,\n",
       "         'city': 8,\n",
       "         'first': 8,\n",
       "         'plan': 8,\n",
       "         'state': 8,\n",
       "         'than': 8,\n",
       "         'were': 8,\n",
       "         'bill': 7,\n",
       "         'made': 7,\n",
       "         'new': 7,\n",
       "         'no': 7,\n",
       "         'should': 7,\n",
       "         'up': 7,\n",
       "         'also': 7,\n",
       "         'federal': 7,\n",
       "         'million': 7,\n",
       "         'other': 7,\n",
       "         'program': 7,\n",
       "         'A': 7,\n",
       "         'but': 7,\n",
       "         'home': 7,\n",
       "         'one': 7,\n",
       "         'pay': 7,\n",
       "         'schools': 7,\n",
       "         'they': 7,\n",
       "         '(': 6,\n",
       "         ')': 6,\n",
       "         'Kennedy': 6,\n",
       "         'committee': 6,\n",
       "         'medical': 6,\n",
       "         'some': 6,\n",
       "         'under': 6,\n",
       "         'Fulton': 6,\n",
       "         'United': 6,\n",
       "         'council': 6,\n",
       "         'law': 6,\n",
       "         'out': 6,\n",
       "         'resolution': 6,\n",
       "         'them': 6,\n",
       "         'County': 6,\n",
       "         'Dallas': 6,\n",
       "         'I': 6,\n",
       "         'Laos': 6,\n",
       "         'States': 6,\n",
       "         'after': 6,\n",
       "         'such': 6,\n",
       "         'these': 6,\n",
       "         'time': 6,\n",
       "         'vote': 6,\n",
       "         'But': 5,\n",
       "         'Sunday': 5,\n",
       "         'being': 5,\n",
       "         'care': 5,\n",
       "         'cases': 5,\n",
       "         'court': 5,\n",
       "         'funds': 5,\n",
       "         'government': 5,\n",
       "         'grants': 5,\n",
       "         'increase': 5,\n",
       "         'local': 5,\n",
       "         'public': 5,\n",
       "         'school': 5,\n",
       "         'take': 5,\n",
       "         'their': 5,\n",
       "         'when': 5,\n",
       "         'In': 5,\n",
       "         'Sen.': 5,\n",
       "         'Senate': 5,\n",
       "         'This': 5,\n",
       "         'aid': 5,\n",
       "         'any': 5,\n",
       "         'campaign': 5,\n",
       "         'charter': 5,\n",
       "         'did': 5,\n",
       "         'director': 5,\n",
       "         'general': 5,\n",
       "         'if': 5,\n",
       "         'into': 5,\n",
       "         'now': 5,\n",
       "         'over': 5,\n",
       "         'per': 5,\n",
       "         'told': 5,\n",
       "         'years': 5,\n",
       "         '1': 4,\n",
       "         'Department': 4,\n",
       "         'Hawksley': 4,\n",
       "         'Republican': 4,\n",
       "         'all': 4,\n",
       "         'called': 4,\n",
       "         'can': 4,\n",
       "         'days': 4,\n",
       "         'most': 4,\n",
       "         'must': 4,\n",
       "         'provide': 4,\n",
       "         'tax': 4,\n",
       "         'two': 4,\n",
       "         'what': 4,\n",
       "         '10': 4,\n",
       "         'Congress': 4,\n",
       "         'Eisenhower': 4,\n",
       "         'Republicans': 4,\n",
       "         'against': 4,\n",
       "         'asked': 4,\n",
       "         'before': 4,\n",
       "         'bonds': 4,\n",
       "         'both': 4,\n",
       "         'county': 4,\n",
       "         'day': 4,\n",
       "         'dollars': 4,\n",
       "         'each': 4,\n",
       "         'go': 4,\n",
       "         'issue': 4,\n",
       "         'may': 4,\n",
       "         'night': 4,\n",
       "         'passed': 4,\n",
       "         'persons': 4,\n",
       "         'present': 4,\n",
       "         'special': 4,\n",
       "         ':': 4,\n",
       "         'ADC': 4,\n",
       "         'Committee': 4,\n",
       "         'Council': 4,\n",
       "         'J.': 4,\n",
       "         'Monday': 4,\n",
       "         'One': 4,\n",
       "         'Rep.': 4,\n",
       "         'There': 4,\n",
       "         'Washington': 4,\n",
       "         'about': 4,\n",
       "         'cent': 4,\n",
       "         'chairman': 4,\n",
       "         'could': 4,\n",
       "         'expected': 4,\n",
       "         'further': 4,\n",
       "         'governor': 4,\n",
       "         'health': 4,\n",
       "         'make': 4,\n",
       "         'meeting': 4,\n",
       "         'people': 4,\n",
       "         'policy': 4,\n",
       "         'possible': 4,\n",
       "         'rule': 4,\n",
       "         'since': 4,\n",
       "         'social': 4,\n",
       "         'work': 4,\n",
       "         'yesterday': 4,\n",
       "         'Austin': 3,\n",
       "         'CD': 3,\n",
       "         'City': 3,\n",
       "         'Communist': 3,\n",
       "         'Dr.': 3,\n",
       "         'Karns': 3,\n",
       "         'Martinelli': 3,\n",
       "         'NATO': 3,\n",
       "         'Party': 3,\n",
       "         'added': 3,\n",
       "         'back': 3,\n",
       "         'because': 3,\n",
       "         'between': 3,\n",
       "         'bills': 3,\n",
       "         'case': 3,\n",
       "         'costs': 3,\n",
       "         'defense': 3,\n",
       "         'do': 3,\n",
       "         'efforts': 3,\n",
       "         'given': 3,\n",
       "         'involved': 3,\n",
       "         'laws': 3,\n",
       "         'need': 3,\n",
       "         'only': 3,\n",
       "         'primary': 3,\n",
       "         'proposal': 3,\n",
       "         'proposed': 3,\n",
       "         'security': 3,\n",
       "         'session': 3,\n",
       "         'system': 3,\n",
       "         'taken': 3,\n",
       "         'those': 3,\n",
       "         'through': 3,\n",
       "         'town': 3,\n",
       "         'ward': 3,\n",
       "         \"'\": 3,\n",
       "         '?': 3,\n",
       "         'B.': 3,\n",
       "         'Clark': 3,\n",
       "         'College': 3,\n",
       "         'Communists': 3,\n",
       "         'Education': 3,\n",
       "         'General': 3,\n",
       "         'Legislature': 3,\n",
       "         'Providence': 3,\n",
       "         'Secretary': 3,\n",
       "         'Socialist': 3,\n",
       "         'Thursday': 3,\n",
       "         'Wexler': 3,\n",
       "         'White': 3,\n",
       "         'another': 3,\n",
       "         'attack': 3,\n",
       "         'board': 3,\n",
       "         'candidate': 3,\n",
       "         'civil': 3,\n",
       "         'cost': 3,\n",
       "         'dental': 3,\n",
       "         'even': 3,\n",
       "         'full-time': 3,\n",
       "         'future': 3,\n",
       "         'get': 3,\n",
       "         'give': 3,\n",
       "         'grand': 3,\n",
       "         'half': 3,\n",
       "         'legislators': 3,\n",
       "         'much': 3,\n",
       "         'our': 3,\n",
       "         'payroll': 3,\n",
       "         'plans': 3,\n",
       "         'president': 3,\n",
       "         'problems': 3,\n",
       "         'property': 3,\n",
       "         'race': 3,\n",
       "         'received': 3,\n",
       "         'report': 3,\n",
       "         'research': 3,\n",
       "         'sales': 3,\n",
       "         'similar': 3,\n",
       "         'statements': 3,\n",
       "         'states': 3,\n",
       "         'study': 3,\n",
       "         'trial': 3,\n",
       "         'very': 3,\n",
       "         'where': 3,\n",
       "         'without': 3,\n",
       "         '24': 2,\n",
       "         'A.': 2,\n",
       "         'Atlanta': 2,\n",
       "         'Bellows': 2,\n",
       "         'Citizens': 2,\n",
       "         'Court': 2,\n",
       "         'Democratic': 2,\n",
       "         'District': 2,\n",
       "         'East': 2,\n",
       "         'Hartsfield': 2,\n",
       "         'Highway': 2,\n",
       "         'His': 2,\n",
       "         'Hughes': 2,\n",
       "         'Jones': 2,\n",
       "         'Jr.': 2,\n",
       "         'Judge': 2,\n",
       "         'Oslo': 2,\n",
       "         'Parkhouse': 2,\n",
       "         'Pelham': 2,\n",
       "         'Reama': 2,\n",
       "         'They': 2,\n",
       "         'Williams': 2,\n",
       "         'act': 2,\n",
       "         'age': 2,\n",
       "         'aged': 2,\n",
       "         'approved': 2,\n",
       "         'attorney': 2,\n",
       "         'ballot': 2,\n",
       "         'become': 2,\n",
       "         'better': 2,\n",
       "         'candidates': 2,\n",
       "         'come': 2,\n",
       "         'counties': 2,\n",
       "         'deaf': 2,\n",
       "         'defendants': 2,\n",
       "         'degree': 2,\n",
       "         'evidence': 2,\n",
       "         'former': 2,\n",
       "         'help': 2,\n",
       "         'hospital': 2,\n",
       "         'information': 2,\n",
       "         'leadership': 2,\n",
       "         'like': 2,\n",
       "         'long': 2,\n",
       "         'means': 2,\n",
       "         'meet': 2,\n",
       "         'members': 2,\n",
       "         'military': 2,\n",
       "         'national': 2,\n",
       "         'next': 2,\n",
       "         'number': 2,\n",
       "         'nursing': 2,\n",
       "         'petition': 2,\n",
       "         'place': 2,\n",
       "         'police': 2,\n",
       "         'policies': 2,\n",
       "         'political': 2,\n",
       "         'precinct': 2,\n",
       "         'problem': 2,\n",
       "         'recommendations': 2,\n",
       "         'recommended': 2,\n",
       "         'rural': 2,\n",
       "         'scheduled': 2,\n",
       "         'several': 2,\n",
       "         'signatures': 2,\n",
       "         'so': 2,\n",
       "         'step': 2,\n",
       "         'taxes': 2,\n",
       "         'teacher': 2,\n",
       "         'then': 2,\n",
       "         'three': 2,\n",
       "         'toward': 2,\n",
       "         'upon': 2,\n",
       "         'voters': 2,\n",
       "         'water': 2,\n",
       "         'we': 2,\n",
       "         'weeks': 2,\n",
       "         '2': 2,\n",
       "         '23d': 2,\n",
       "         '30': 2,\n",
       "         '4': 2,\n",
       "         'Assembly': 2,\n",
       "         'Central': 2,\n",
       "         'Charles': 2,\n",
       "         'Cotten': 2,\n",
       "         'Criminal': 2,\n",
       "         'Cuba': 2,\n",
       "         'Daniel': 2,\n",
       "         'Friday': 2,\n",
       "         'George': 2,\n",
       "         'Georgia': 2,\n",
       "         \"Georgia's\": 2,\n",
       "         'Gov.': 2,\n",
       "         'Group': 2,\n",
       "         'Grover': 2,\n",
       "         'Hospital': 2,\n",
       "         'Island': 2,\n",
       "         'James': 2,\n",
       "         'John': 2,\n",
       "         'Johnston': 2,\n",
       "         'Lao': 2,\n",
       "         'M.': 2,\n",
       "         'Mitchell': 2,\n",
       "         'Notte': 2,\n",
       "         'Rhode': 2,\n",
       "         'Some': 2,\n",
       "         'Vandiver': 2,\n",
       "         'action': 2,\n",
       "         'address': 2,\n",
       "         'ago': 2,\n",
       "         'amount': 2,\n",
       "         'annual': 2,\n",
       "         'appointment': 2,\n",
       "         'banks': 2,\n",
       "         'believes': 2,\n",
       "         'charged': 2,\n",
       "         'child': 2,\n",
       "         'children': 2,\n",
       "         'cities': 2,\n",
       "         'citizens': 2,\n",
       "         'commission': 2,\n",
       "         'community': 2,\n",
       "         'concerned': 2,\n",
       "         'conference': 2,\n",
       "         'courses': 2,\n",
       "         'declared': 2,\n",
       "         'despite': 2,\n",
       "         'does': 2,\n",
       "         'education': 2,\n",
       "         'elected': 2,\n",
       "         'enabling': 2,\n",
       "         'enough': 2,\n",
       "         'establishment': 2,\n",
       "         'ever': 2,\n",
       "         'five': 2,\n",
       "         'got': 2,\n",
       "         'group': 2,\n",
       "         'heard': 2,\n",
       "         'here': 2,\n",
       "         'high': 2,\n",
       "         'judges': 2,\n",
       "         'large': 2,\n",
       "         'later': 2,\n",
       "         'left': 2,\n",
       "         'legislation': 2,\n",
       "         'limited': 2,\n",
       "         'listed': 2,\n",
       "         'major': 2,\n",
       "         'making': 2,\n",
       "         'man': 2,\n",
       "         'matter': 2,\n",
       "         'mean': 2,\n",
       "         'message': 2,\n",
       "         'might': 2,\n",
       "         'millions': 2,\n",
       "         'money': 2,\n",
       "         'movement': 2,\n",
       "         'needs': 2,\n",
       "         'never': 2,\n",
       "         'obtain': 2,\n",
       "         'obtained': 2,\n",
       "         'opposition': 2,\n",
       "         'order': 2,\n",
       "         'paid': 2,\n",
       "         'parties': 2,\n",
       "         'past': 2,\n",
       "         'patient': 2,\n",
       "         'person': 2,\n",
       "         'petitions': 2,\n",
       "         'precincts': 2,\n",
       "         'programs': 2,\n",
       "         'proposals': 2,\n",
       "         'question': 2,\n",
       "         'railroad': 2,\n",
       "         'receive': 2,\n",
       "         'relations': 2,\n",
       "         'reported': 2,\n",
       "         'residents': 2,\n",
       "         'right': 2,\n",
       "         'service': 2,\n",
       "         'services': 2,\n",
       "         'set': 2,\n",
       "         'seven': 2,\n",
       "         'soon': 2,\n",
       "         'source': 2,\n",
       "         'studied': 2,\n",
       "         'suggested': 2,\n",
       "         'term': 2,\n",
       "         'track': 2,\n",
       "         'use': 2,\n",
       "         'voted': 2,\n",
       "         'votes': 2,\n",
       "         'week': 2,\n",
       "         'welfare': 2,\n",
       "         'whether': 2,\n",
       "         'willing': 2,\n",
       "         'within': 2,\n",
       "         'workers': 2,\n",
       "         'working': 2,\n",
       "         'yet': 2,\n",
       "         '13': 2,\n",
       "         '1910': 2,\n",
       "         '20': 2,\n",
       "         '3': 2,\n",
       "         '65': 2,\n",
       "         '70': 2,\n",
       "         'After': 2,\n",
       "         'American': 2,\n",
       "         'Authority': 2,\n",
       "         'Barber': 2,\n",
       "         'Berlin': 2,\n",
       "         'Berry': 2,\n",
       "         'Bourcier': 2,\n",
       "         'Bush': 2,\n",
       "         'C.': 2,\n",
       "         'Club': 2,\n",
       "         'Cook': 2,\n",
       "         'D.': 2,\n",
       "         'Davis': 2,\n",
       "         'E.': 2,\n",
       "         'Executive': 2,\n",
       "         'Falls': 2,\n",
       "         'Feb.': 2,\n",
       "         'Foreign': 2,\n",
       "         'Fort': 2,\n",
       "         'GOP': 2,\n",
       "         'Geneva': 2,\n",
       "         'Grant': 2,\n",
       "         'H.': 2,\n",
       "         'Health': 2,\n",
       "         'Houston': 2,\n",
       "         'If': 2,\n",
       "         'Jan.': 2,\n",
       "         'July': 2,\n",
       "         'March': 2,\n",
       "         'Martin': 2,\n",
       "         'Massachusetts': 2,\n",
       "         'Miss': 2,\n",
       "         'New': 2,\n",
       "         'Nixon': 2,\n",
       "         'Oklahoma': 2,\n",
       "         'Organization': 2,\n",
       "         'Other': 2,\n",
       "         'P.': 2,\n",
       "         'Paris': 2,\n",
       "         'Parsons': 2,\n",
       "         \"President's\": 2,\n",
       "         'R.': 2,\n",
       "         'Ratcliff': 2,\n",
       "         'Republicanism': 2,\n",
       "         'Roberts': 2,\n",
       "         \"Rusk's\": 2,\n",
       "         'Sam': 2,\n",
       "         'School': 2,\n",
       "         'Souvanna': 2,\n",
       "         'Soviet': 2,\n",
       "         'Superior': 2,\n",
       "         'These': 2,\n",
       "         'University': 2,\n",
       "         'W.': 2,\n",
       "         'Welfare': 2,\n",
       "         'When': 2,\n",
       "         'While': 2,\n",
       "         'William': 2,\n",
       "         'accept': 2,\n",
       "         'achieve': 2,\n",
       "         'actions': 2,\n",
       "         'again': 2,\n",
       "         'alliance': 2,\n",
       "         'allowed': 2,\n",
       "         'amendment': 2,\n",
       "         'announced': 2,\n",
       "         'anonymous': 2,\n",
       "         'apparent': 2,\n",
       "         'approval': 2,\n",
       "         'argued': 2,\n",
       "         'ask': 2,\n",
       "         'attend': 2,\n",
       "         'audience': 2,\n",
       "         'authority': 2,\n",
       "         'bankers': 2,\n",
       "         'billion': 2,\n",
       "         'blue': 2,\n",
       "         'bond': 2,\n",
       "         'brought': 2,\n",
       "         'business': 2,\n",
       "         'calls': 2,\n",
       "         'career': 2,\n",
       "         'carry': 2,\n",
       "         'caused': 2,\n",
       "         'causes': 2,\n",
       "         'changes': 2,\n",
       "         'charge': 2,\n",
       "         'chief': 2,\n",
       "         'college': 2,\n",
       "         'companies': 2,\n",
       "         'constituted': 2,\n",
       "         'constitutional': 2,\n",
       "         'construction': 2,\n",
       "         'controversy': 2,\n",
       "         'countries': 2,\n",
       "         'critical': 2,\n",
       "         'debate': 2,\n",
       "         'denied': 2,\n",
       "         'designed': 2,\n",
       "         \"didn't\": 2,\n",
       "         'discrimination': 2,\n",
       "         'doctor': 2,\n",
       "         'dollar': 2,\n",
       "         'down': 2,\n",
       "         'during': 2,\n",
       "         'early': 2,\n",
       "         'effective': 2,\n",
       "         'eight': 2,\n",
       "         'eliminating': 2,\n",
       "         'employment': 2,\n",
       "         'enforcement': 2,\n",
       "         'essential': 2,\n",
       "         'estimated': 2,\n",
       "         'fact': 2,\n",
       "         'fair': 2,\n",
       "         'family': 2,\n",
       "         'felt': 2,\n",
       "         'fight': 2,\n",
       "         'find': 2,\n",
       "         'follow': 2,\n",
       "         'force': 2,\n",
       "         'forces': 2,\n",
       "         'foreign': 2,\n",
       "         'four': 2,\n",
       "         'free': 2,\n",
       "         'fund': 2,\n",
       "         'generally': 2,\n",
       "         'going': 2,\n",
       "         'greater': 2,\n",
       "         'gubernatorial': 2,\n",
       "         'hearing': 2,\n",
       "         'held': 2,\n",
       "         'highway': 2,\n",
       "         'him': 2,\n",
       "         'himself': 2,\n",
       "         'hold': 2,\n",
       "         'homes': 2,\n",
       "         'hours': 2,\n",
       "         'however': 2,\n",
       "         'immediate': 2,\n",
       "         'including': 2,\n",
       "         'indicated': 2,\n",
       "         'instead': 2,\n",
       "         'insurance': 2,\n",
       "         'interested': 2,\n",
       "         'investigation': 2,\n",
       "         'irregularities': 2,\n",
       "         'issued': 2,\n",
       "         'juvenile': 2,\n",
       "         'little': 2,\n",
       "         'manner': 2,\n",
       "         'many': 2,\n",
       "         'matching': 2,\n",
       "         \"mayor's\": 2,\n",
       "         'me': 2,\n",
       "         'medicine': 2,\n",
       "         'mention': 2,\n",
       "         'mind': 2,\n",
       "         'months': 2,\n",
       "         'my': 2,\n",
       "         \"nation's\": 2,\n",
       "         'nine': 2,\n",
       "         'none': 2,\n",
       "         'notice': 2,\n",
       "         'nuclear': 2,\n",
       "         'offenses': 2,\n",
       "         'ones': 2,\n",
       "         'opinion': 2,\n",
       "         'ordinance': 2,\n",
       "         'organization': 2,\n",
       "         'own': 2,\n",
       "         'party': 2,\n",
       "         'personal': 2,\n",
       "         'poll': 2,\n",
       "         'polls': 2,\n",
       "         'practices': 2,\n",
       "         'previous': 2,\n",
       "         'principal': 2,\n",
       "         'procedures': 2,\n",
       "         'raises': 2,\n",
       "         'ready': 2,\n",
       "         'real': 2,\n",
       "         'recent': 2,\n",
       "         'recommend': 2,\n",
       "         'reduce': 2,\n",
       "         'remark': 2,\n",
       "         'required': 2,\n",
       "         'rescue': 2,\n",
       "         'retired': 2,\n",
       "         'retirement': 2,\n",
       "         'scholarships': 2,\n",
       "         'semester': 2,\n",
       "         'senator': 2,\n",
       "         'serious': 2,\n",
       "         'served': 2,\n",
       "         'situation': 2,\n",
       "         'spent': 2,\n",
       "         'spokesmen': 2,\n",
       "         'statement': 2,\n",
       "         'steps': 2,\n",
       "         'still': 2,\n",
       "         'superintendent': 2,\n",
       "         'support': 2,\n",
       "         'teaching': 2,\n",
       "         'test': 2,\n",
       "         'themselves': 2,\n",
       "         'today': 2,\n",
       "         'too': 2,\n",
       "         'trouble': 2,\n",
       "         'trucks': 2,\n",
       "         'understanding': 2,\n",
       "         'urged': 2,\n",
       "         'want': 2,\n",
       "         'went': 2,\n",
       "         'while': 2,\n",
       "         'whom': 2,\n",
       "         'worth': 2,\n",
       "         '$1,000': 1,\n",
       "         '$1,500': 1,\n",
       "         '$10': 1,\n",
       "         '$37': 1,\n",
       "         '&': 1,\n",
       "         '18': 1,\n",
       "         '1961': 1,\n",
       "         '1961-62': 1,\n",
       "         '1963': 1,\n",
       "         '4-year': 1,\n",
       "         '50': 1,\n",
       "         '58th': 1,\n",
       "         '6': 1,\n",
       "         '8': 1,\n",
       "         '9': 1,\n",
       "         ';': 1,\n",
       "         'Acting': 1,\n",
       "         'Allen': 1,\n",
       "         'Angola': 1,\n",
       "         'April': 1,\n",
       "         'Army': 1,\n",
       "         'Association': 1,\n",
       "         'At': 1,\n",
       "         \"Atlanta's\": 1,\n",
       "         'Attorney': 1,\n",
       "         'Aug.': 1,\n",
       "         'Bill': 1,\n",
       "         'Blue': 1,\n",
       "         'Both': 1,\n",
       "         'Caldwell': 1,\n",
       "         'Calls': 1,\n",
       "         'Chairman': 1,\n",
       "         'Chapman': 1,\n",
       "         'Chief': 1,\n",
       "         'Co.': 1,\n",
       "         'Colquitt': 1,\n",
       "         'Cox': 1,\n",
       "         'Crime': 1,\n",
       "         'Delinquency': 1,\n",
       "         'Denton': 1,\n",
       "         \"Department's\": 1,\n",
       "         'Dumont': 1,\n",
       "         'During': 1,\n",
       "         'El': 1,\n",
       "         'Felix': 1,\n",
       "         'Fire': 1,\n",
       "         'Frank': 1,\n",
       "         'Full': 1,\n",
       "         'Griffin': 1,\n",
       "         'Halleck': 1,\n",
       "         'Hays': 1,\n",
       "         'Henry': 1,\n",
       "         'Hotel': 1,\n",
       "         'How': 1,\n",
       "         'Institute': 1,\n",
       "         'Ivan': 1,\n",
       "         'Joe': 1,\n",
       "         'Juvenile': 1,\n",
       "         'Kan.': 1,\n",
       "         \"Kennedy's\": 1,\n",
       "         'L.': 1,\n",
       "         'Leader': 1,\n",
       "         'Louis': 1,\n",
       "         'Mayor': 1,\n",
       "         'Miller': 1,\n",
       "         'Nam': 1,\n",
       "         'North': 1,\n",
       "         'Officials': 1,\n",
       "         'On': 1,\n",
       "         'Only': 1,\n",
       "         'Ordinary': 1,\n",
       "         'Paradise': 1,\n",
       "         'Paso': 1,\n",
       "         'Pathet': 1,\n",
       "         'Phouma': 1,\n",
       "         'Policies': 1,\n",
       "         'Port': 1,\n",
       "         'Presidential': 1,\n",
       "         'Prince': 1,\n",
       "         'Raymond': 1,\n",
       "         'Research': 1,\n",
       "         'Richard': 1,\n",
       "         'Ridge': 1,\n",
       "         'Riverside': 1,\n",
       "         'Roads': 1,\n",
       "         'Robert': 1,\n",
       "         'Rural': 1,\n",
       "         'SEATO': 1,\n",
       "         'Salinger': 1,\n",
       "         'San': 1,\n",
       "         'Science': 1,\n",
       "         'Scott': 1,\n",
       "         'Seekonk': 1,\n",
       "         'Senator': 1,\n",
       "         'Sept.': 1,\n",
       "         'September': 1,\n",
       "         'Several': 1,\n",
       "         'Sherman': 1,\n",
       "         'South': 1,\n",
       "         'Southeast': 1,\n",
       "         'Technology': 1,\n",
       "         'Thailand': 1,\n",
       "         'That': 1,\n",
       "         'Town': 1,\n",
       "         'Treaty': 1,\n",
       "         'Tuesday': 1,\n",
       "         'Two': 1,\n",
       "         'U.S.': 1,\n",
       "         'Under': 1,\n",
       "         'Union': 1,\n",
       "         'Viet': 1,\n",
       "         'Wayne': 1,\n",
       "         'We': 1,\n",
       "         'Weatherford': 1,\n",
       "         'West': 1,\n",
       "         'Worth': 1,\n",
       "         'York': 1,\n",
       "         'You': 1,\n",
       "         'able': 1,\n",
       "         'absorbed': 1,\n",
       "         'acceptable': 1,\n",
       "         'according': 1,\n",
       "         'addition': 1,\n",
       "         'additional': 1,\n",
       "         'adjournment': 1,\n",
       "         'adopted': 1,\n",
       "         'advantage': 1,\n",
       "         'advised': 1,\n",
       "         'advisement': 1,\n",
       "         'agreed': 1,\n",
       "         'airport': 1,\n",
       "         'allow': 1,\n",
       "         'ally': 1,\n",
       "         'almost': 1,\n",
       "         'alone': 1,\n",
       "         'along': 1,\n",
       "         'alternative': 1,\n",
       "         'among': 1,\n",
       "         'apparently': 1,\n",
       "         'approve': 1,\n",
       "         'area': 1,\n",
       "         'areas': 1,\n",
       "         'arms': 1,\n",
       "         'arrests': 1,\n",
       "         'aside': 1,\n",
       "         'assembly': 1,\n",
       "         'assistance': 1,\n",
       "         'assistant': 1,\n",
       "         'attempt': 1,\n",
       "         'authorities': 1,\n",
       "         'authorizing': 1,\n",
       "         'available': 1,\n",
       "         'bank': 1,\n",
       "         'base': 1,\n",
       "         'basic': 1,\n",
       "         'basis': 1,\n",
       "         'begin': 1,\n",
       "         'benefit': 1,\n",
       "         'benefits': 1,\n",
       "         'betting': 1,\n",
       "         'big': 1,\n",
       "         'bit': 1,\n",
       "         'boos': 1,\n",
       "         'boost': 1,\n",
       "         'brokers': 1,\n",
       "         'budget': 1,\n",
       "         'build': 1,\n",
       "         'building': 1,\n",
       "         'burglary': 1,\n",
       "         'cabinet': 1,\n",
       "         'call': 1,\n",
       "         'came': 1,\n",
       "         'cannot': 1,\n",
       "         'canvassers': 1,\n",
       "         'capital': 1,\n",
       "         'carcass': 1,\n",
       "         'cease-fire': 1,\n",
       "         'certain': 1,\n",
       "         'certainly': 1,\n",
       "         'certificate': 1,\n",
       "         'choice': 1,\n",
       "         'clear': 1,\n",
       "         'client': 1,\n",
       "         'combined': 1,\n",
       "         'coming': 1,\n",
       "         'commented': 1,\n",
       "         'confidence': 1,\n",
       "         'conflict': 1,\n",
       "         'connection': 1,\n",
       "         'consulted': 1,\n",
       "         'continue': 1,\n",
       "         'contracts': 1,\n",
       "         'conventional': 1,\n",
       "         'coolest': 1,\n",
       "         'cooperation': 1,\n",
       "         'correspondents': 1,\n",
       "         'costly': 1,\n",
       "         'country': 1,\n",
       "         'couple': 1,\n",
       "         'courts': 1,\n",
       "         'cover': 1,\n",
       "         'credit': 1,\n",
       "         'crisis': 1,\n",
       "         'danger': 1,\n",
       "         'date': 1,\n",
       "         'decisions': 1,\n",
       "         'defeated': 1,\n",
       "         'delay': 1,\n",
       "         'delegation': 1,\n",
       "         'demand': 1,\n",
       "         'department': 1,\n",
       "         'departments': 1,\n",
       "         'dependency': 1,\n",
       "         'deputies': 1,\n",
       "         'detailed': 1,\n",
       "         'details': 1,\n",
       "         'developed': 1,\n",
       "         'development': 1,\n",
       "         'diplomatic': 1,\n",
       "         'directed': 1,\n",
       "         'disagreed': 1,\n",
       "         'disclosure': 1,\n",
       "         'districts': 1,\n",
       "         'divorce': 1,\n",
       "         'done': 1,\n",
       "         'drafts': 1,\n",
       "         'duty': 1,\n",
       "         'earlier': 1,\n",
       "         'economic': 1,\n",
       "         'effect': 1,\n",
       "         'effort': 1,\n",
       "         'elaborate': 1,\n",
       "         'eligible': 1,\n",
       "         'employed': 1,\n",
       "         'end': 1,\n",
       "         'endorse': 1,\n",
       "         'enforced': 1,\n",
       "         'enter': 1,\n",
       "         'enterprise': 1,\n",
       "         'escheat': 1,\n",
       "         'establish': 1,\n",
       "         'event': 1,\n",
       "         'except': 1,\n",
       "         'exception': 1,\n",
       "         'executive': 1,\n",
       "         'expects': 1,\n",
       "         'expense': 1,\n",
       "         'explained': 1,\n",
       "         'expressed': 1,\n",
       "         'facilities': 1,\n",
       "         'factor': 1,\n",
       "         'far': 1,\n",
       "         'favor': 1,\n",
       "         'feel': 1,\n",
       "         'field': 1,\n",
       "         'finally': 1,\n",
       "         'finance': 1,\n",
       "         'fine': 1,\n",
       "         'fire': 1,\n",
       "         'firm': 1,\n",
       "         'floor': 1,\n",
       "         'forced': 1,\n",
       "         'form': 1,\n",
       "         'formally': 1,\n",
       "         'formula': 1,\n",
       "         'found': 1,\n",
       "         'gas': 1,\n",
       "         'gave': 1,\n",
       "         'getting': 1,\n",
       "         'global': 1,\n",
       "         \"governor's\": 1,\n",
       "         'grant': 1,\n",
       "         'great': 1,\n",
       "         'groups': 1,\n",
       "         'growth': 1,\n",
       "         'guilt': 1,\n",
       "         'hand': 1,\n",
       "         'having': 1,\n",
       "         'head': 1,\n",
       "         'hear': 1,\n",
       "         'hearings': 1,\n",
       "         'heart': 1,\n",
       "         'higher': 1,\n",
       "         'hoped': 1,\n",
       "         'horse': 1,\n",
       "         'hospitals': 1,\n",
       "         'house': 1,\n",
       "         'housing': 1,\n",
       "         'how': 1,\n",
       "         'idea': 1,\n",
       "         'illness': 1,\n",
       "         'impossible': 1,\n",
       "         ...})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(unigram_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ba2f74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, word2index):\n",
    "    unk = word2index[\"<UNK>\"]\n",
    "    idxs = [word2index.get(w, unk) for w in seq]\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "def negative_sampling(targets, unigram_table, k):\n",
    "    if targets.dim() == 2:\n",
    "        targets_1d = targets.squeeze(1)\n",
    "    else:\n",
    "        targets_1d = targets\n",
    "\n",
    "    batch_size = targets_1d.size(0)\n",
    "    neg_samples = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        target_index = targets_1d[i].item()\n",
    "        nsample = []\n",
    "\n",
    "        while len(nsample) < k:\n",
    "            neg_word = random.choice(unigram_table)  # sampled token (string)\n",
    "            neg_idx = word2index[neg_word]\n",
    "            if neg_idx == target_index:\n",
    "                continue\n",
    "            nsample.append(neg_word)\n",
    "\n",
    "        neg_samples.append(prepare_sequence(nsample, word2index).view(1, -1))  # [1,k]\n",
    "\n",
    "    return torch.cat(neg_samples, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "821fd4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipgramNegSampling(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(SkipgramNegSampling, self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, emb_size)  # center embedding (v)\n",
    "        self.embedding_u = nn.Embedding(vocab_size, emb_size)  # outside embedding (u)\n",
    "        self.logsigmoid = nn.LogSigmoid()\n",
    "\n",
    "    def forward(self, center_words, target_words, negative_words):\n",
    "\n",
    "        # Look up embeddings\n",
    "        center_embeds = self.embedding_v(center_words)            # [B, 1, D]\n",
    "        target_embeds = self.embedding_u(target_words)            # [B, 1, D]\n",
    "\n",
    "        # NOTE: negative sign is applied here (same as professor)\n",
    "        neg_embeds = -self.embedding_u(negative_words)            # [B, K, D]\n",
    "\n",
    "        # Positive score: u_target  v_center -> [B, 1]\n",
    "        positive_score = target_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        # [B,1,D] @ [B,D,1] -> [B,1,1] -> squeeze -> [B,1]\n",
    "\n",
    "        # Negative score: (-u_neg)  v_center -> [B, K, 1]\n",
    "        negative_score = neg_embeds.bmm(center_embeds.transpose(1, 2))\n",
    "        # [B,K,D] @ [B,D,1] -> [B,K,1]\n",
    "\n",
    "        # log (pos) + sum_k log (neg)\n",
    "        loss = self.logsigmoid(positive_score) + torch.sum(self.logsigmoid(negative_score), dim=1)\n",
    "        # positive_score: [B,1]\n",
    "        # negative_score: [B,K,1] -> logsigmoid keeps shape -> sum over K -> [B,1]\n",
    "\n",
    "        return -torch.mean(loss)  # scalar\n",
    "\n",
    "    def prediction(self, inputs):\n",
    "        return self.embedding_v(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e158ba",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2f72b97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000 | cost: 31.965076 | time: 13.34 ms\n",
      "Epoch: 2000 | cost: 24.614790 | time: 13.19 ms\n",
      "Epoch: 3000 | cost: 21.269592 | time: 13.15 ms\n",
      "Epoch: 4000 | cost: 14.814827 | time: 70.79 ms\n",
      "Epoch: 5000 | cost: 10.852766 | time: 68.66 ms\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "batch_size     = 64      # use 2 for debugging; increase for real training\n",
    "embedding_size = 100     # 2 only for plotting; use 50/100/200 for better quality\n",
    "window_size    = 2       # Task 1 default\n",
    "num_neg        = 10      # k negative samples (prof uses 10)\n",
    "\n",
    "model_neg = SkipgramNegSampling(vocab_size, embedding_size).to(device)\n",
    "optimizer = optim.Adam(model_neg.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 5000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    # sample skip-gram pairs\n",
    "    input_batch, target_batch = random_batch(batch_size, corpus, window_size=window_size)\n",
    "\n",
    "    # to tensors\n",
    "    input_tensor  = torch.LongTensor(input_batch).to(device)   # [B,1]\n",
    "    target_tensor = torch.LongTensor(target_batch).to(device)  # [B,1]\n",
    "\n",
    "    # sample negatives (on CPU first, then move to device)\n",
    "    negs = negative_sampling(target_tensor.cpu(), unigram_table, num_neg).to(device)  # [B,k]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = model_neg(input_tensor, target_tensor, negs)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        # printing ms because per-epoch can be < 1 sec\n",
    "        print(f\"Epoch: {epoch+1} | cost: {loss.item():.6f} | time: {(end-start)*1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f0a45e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed_neg_sample(word):\n",
    "    idx = word2index.get(word, word2index[\"<UNK>\"])\n",
    "    idx_tensor = torch.LongTensor([idx]).to(device)\n",
    "\n",
    "    v_embed = model_neg.embedding_v(idx_tensor)  # [1, D]\n",
    "    u_embed = model_neg.embedding_u(idx_tensor)  # [1, D]\n",
    "\n",
    "    word_embed = (v_embed + u_embed) / 2.0\n",
    "    return word_embed.squeeze(0).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "253721cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "election = get_embed_neg_sample(\"election\")\n",
    "vote = get_embed_neg_sample(\"vote\")\n",
    "campaign = get_embed_neg_sample(\"campaign\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4cc02401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "election vs vote:        -0.0587\n",
      "election vs campaign:   -0.0373\n",
      "election vs election:   1.0000\n"
     ]
    }
   ],
   "source": [
    "print(f\"election vs vote:        {cos_sim(election, vote):.4f}\")\n",
    "print(f\"election vs campaign:   {cos_sim(election, campaign):.4f}\")\n",
    "print(f\"election vs election:   {cos_sim(election, election):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "daf4076e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "torch.save(model_neg.state_dict(), 'model/skipgram_neg_model.pth')\n",
    "pickle.dump(model_neg, open('model/skipgram_neg.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b31b01",
   "metadata": {},
   "source": [
    "<h1>Glove<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fecf7978",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "#Counting unigram frequencies\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "X_i = Counter(flatten(corpus)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d0ad6968",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building skip-grams and co-occurrence counts X_ik with dynamic window size\n",
    "def build_cooccurrence(corpus, window_size=2):\n",
    "    skip_grams = []\n",
    "    X_ik_skipgram = Counter()\n",
    "\n",
    "    for sent in corpus:\n",
    "        # skip edges that don't have full context window\n",
    "        for i in range(window_size, len(sent) - window_size):\n",
    "            target = sent[i]\n",
    "\n",
    "            # context within +/- window_size (both sides)\n",
    "            for w in range(1, window_size + 1):\n",
    "                left = sent[i - w]\n",
    "                right = sent[i + w]\n",
    "\n",
    "                # add (target, context)\n",
    "                skip_grams.append((target, left))\n",
    "                skip_grams.append((target, right))\n",
    "\n",
    "                X_ik_skipgram[(target, left)] += 1\n",
    "                X_ik_skipgram[(target, right)] += 1\n",
    "\n",
    "    return X_ik_skipgram, skip_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "91779d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 2\n",
    "X_ik_skipgram, skip_grams = build_cooccurrence(corpus, window_size=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4d6eaf1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built co-occurrence pairs: 30394 (window_size=2)\n"
     ]
    }
   ],
   "source": [
    "#GloVe weighting function f(x)\n",
    "def weighting_count(x_ij, x_max=100, alpha=0.75):\n",
    "    if x_ij < x_max:\n",
    "        return (x_ij / x_max) ** alpha\n",
    "    return 1.0\n",
    "\n",
    "X_ik = {}          \n",
    "weighting_dic = {}  \n",
    "\n",
    "\n",
    "for (wi, wj), cnt in X_ik_skipgram.items():\n",
    "    c = cnt + 1\n",
    "    X_ik[(wi, wj)] = c\n",
    "    X_ik[(wj, wi)] = c  \n",
    "\n",
    "    w_val = weighting_count(c) \n",
    "    weighting_dic[(wi, wj)] = w_val\n",
    "    weighting_dic[(wj, wi)] = w_val\n",
    "\n",
    "print(f\"Built co-occurrence pairs: {len(X_ik)} (window_size={window_size})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a62e0493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "import math\n",
    "def random_batch_glove(batch_size, skip_grams, X_ik, weighting_dic, word2index):\n",
    "    unk = word2index[\"<UNK>\"]\n",
    "\n",
    "    # sample random indices\n",
    "    batch_size = min(batch_size, len(skip_grams))\n",
    "    random_index = np.random.choice(range(len(skip_grams)), batch_size, replace=False)\n",
    "\n",
    "    random_inputs = []\n",
    "    random_labels = []\n",
    "    random_coocs = []\n",
    "    random_weightings = []\n",
    "\n",
    "    for idx in random_index:\n",
    "        wi, wj = skip_grams[idx]\n",
    "\n",
    "        wi_id = word2index.get(wi, unk)\n",
    "        wj_id = word2index.get(wj, unk)\n",
    "\n",
    "        random_inputs.append([wi_id])\n",
    "        random_labels.append([wj_id])\n",
    "\n",
    "        # co-occurrence count (already symmetric in X_ik)\n",
    "        cooc = X_ik.get((wi, wj), 1)\n",
    "        random_coocs.append([math.log(cooc)])  # GloVe uses log(X_ij)\n",
    "\n",
    "        # weighting value\n",
    "        wt = weighting_dic.get((wi, wj), weighting_count(1))\n",
    "        random_weightings.append([wt])\n",
    "\n",
    "    return (np.array(random_inputs),\n",
    "            np.array(random_labels),\n",
    "            np.array(random_coocs, dtype=np.float32),\n",
    "            np.array(random_weightings, dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "298d8c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [[1946]\n",
      " [  53]]\n",
      "Target: [[1]\n",
      " [0]]\n",
      "Cooc (log): [[1.0986123]\n",
      " [2.1972246]]\n",
      "Weighting: [[0.07208434]\n",
      " [0.16431677]]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "input_b, target_b, cooc_b, weight_b = random_batch_glove(\n",
    "    batch_size, skip_grams, X_ik, weighting_dic, word2index\n",
    ")\n",
    "\n",
    "print(\"Input:\", input_b)\n",
    "print(\"Target:\", target_b)\n",
    "print(\"Cooc (log):\", cooc_b)\n",
    "print(\"Weighting:\", weight_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "86f22d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloVe(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(GloVe, self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, embed_size)  # center\n",
    "        self.embedding_u = nn.Embedding(vocab_size, embed_size)  # outside\n",
    "\n",
    "        self.v_bias = nn.Embedding(vocab_size, 1)\n",
    "        self.u_bias = nn.Embedding(vocab_size, 1)\n",
    "\n",
    "    def forward(self, center_words, target_words, coocs, weighting):\n",
    "        center_embeds = self.embedding_v(center_words)  # [B,1,D]\n",
    "        target_embeds = self.embedding_u(target_words)  # [B,1,D]\n",
    "\n",
    "        center_bias = self.v_bias(center_words).squeeze(1)  # [B,1]\n",
    "        target_bias = self.u_bias(target_words).squeeze(1)  # [B,1]\n",
    "\n",
    "        inner_product = target_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)  # [B,1]\n",
    "\n",
    "        # coocs is log(X_ij), weighting is f(X_ij)\n",
    "        loss = weighting * torch.pow(inner_product + center_bias + target_bias - coocs, 2)\n",
    "\n",
    "        return torch.mean(loss)  # better than sum for comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283f6ba4",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cc38acd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GloVe training setup\n",
    "\n",
    "batch_size     = 10  # mini-batch size\n",
    "embedding_size = 2   # small for visualization; increase later for real training\n",
    "\n",
    "model_glove = GloVe(vocab_size, embedding_size).to(device)\n",
    "\n",
    "# GloVe defines its own loss internally (weighted MSE),\n",
    "# so NO external criterion is needed.\n",
    "optimizer = optim.Adam(model_glove.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f5b0d5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000 | cost: 0.236655 | time: 1.56 ms\n",
      "Epoch: 2000 | cost: 0.019608 | time: 1.54 ms\n",
      "Epoch: 3000 | cost: 0.177698 | time: 1.51 ms\n",
      "Epoch: 4000 | cost: 0.136542 | time: 1.52 ms\n",
      "Epoch: 5000 | cost: 0.134668 | time: 1.55 ms\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Getting a random GloVe batch \n",
    "    input_batch, target_batch, cooc_batch, weighting_batch = random_batch_glove(\n",
    "        batch_size,\n",
    "        skip_grams,\n",
    "        X_ik,\n",
    "        weighting_dic,\n",
    "        word2index\n",
    "    )\n",
    "\n",
    "    # Converting to tensors \n",
    "    input_batch     = torch.LongTensor(input_batch).to(device)     # [B,1]\n",
    "    target_batch    = torch.LongTensor(target_batch).to(device)    # [B,1]\n",
    "    cooc_batch      = torch.FloatTensor(cooc_batch).to(device)     # [B,1] (log X_ij)\n",
    "    weighting_batch = torch.FloatTensor(weighting_batch).to(device)# [B,1]\n",
    "\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = model_glove(input_batch, target_batch, cooc_batch, weighting_batch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    # ---- logging ----\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(\n",
    "            f\"Epoch: {epoch + 1} | \"\n",
    "            f\"cost: {loss.item():.6f} | \"\n",
    "            f\"time: {(end - start) * 1000:.2f} ms\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9afda545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model_glove.state_dict(), 'model/glove_model.pth')\n",
    "# save the model using pickle\n",
    "pickle.dump(model_glove, open('model/glove.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "98d57669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed_glove(word):\n",
    "    idx = word2index.get(word, word2index[\"<UNK>\"])\n",
    "    idx_tensor = torch.LongTensor([idx]).to(device)\n",
    "\n",
    "    v_embed = model_glove.embedding_v(idx_tensor)  # [1, D]\n",
    "    u_embed = model_glove.embedding_u(idx_tensor)  # [1, D]\n",
    "\n",
    "    word_embed = (v_embed + u_embed) / 2.0\n",
    "    return word_embed.squeeze(0).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a142ddb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "election = get_embed_glove(\"election\")\n",
    "vote = get_embed_glove(\"vote\")\n",
    "campaign = get_embed_glove(\"campaign\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dd018578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "election vs vote:        0.2825\n",
      "election vs campaign:   -0.5699\n",
      "election vs election:   1.0000\n"
     ]
    }
   ],
   "source": [
    "print(f\"election vs vote:        {cos_sim(election, vote):.4f}\")\n",
    "print(f\"election vs campaign:   {cos_sim(election, campaign):.4f}\")\n",
    "print(f\"election vs election:   {cos_sim(election, election):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1bd5823d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "#you have to put this file in some python/gensim directory; just run it and it will inform where to put....\n",
    "glove_file = 'glove.6B/glove.6B.100d.txt'\n",
    "model_genism = KeyedVectors.load_word2vec_format(glove_file, binary=False, no_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a5da9eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'government' and 'administration': 0.7937\n",
      "Similarity between 'election' and 'vote': 0.8465\n",
      "President - Man + Woman = vice\n",
      "Government - Country + State = federal\n"
     ]
    }
   ],
   "source": [
    "# Example: Word similarity\n",
    "\n",
    "similarity = model_genism.similarity('government', 'administration')\n",
    "print(f\"Similarity between 'government' and 'administration': {similarity:.4f}\")\n",
    "\n",
    "similarity = model_genism.similarity('election', 'vote')\n",
    "print(f\"Similarity between 'election' and 'vote': {similarity:.4f}\")\n",
    "\n",
    "\n",
    "# Example: Word analogy \n",
    "result = model_genism.most_similar(\n",
    "    positive=['president', 'woman'],\n",
    "    negative=['man'],\n",
    "    topn=1\n",
    ")\n",
    "print(\"President - Man + Woman =\", result[0][0])\n",
    "\n",
    "\n",
    "result = model_genism.most_similar(\n",
    "    positive=['government', 'state'],\n",
    "    negative=['country'],\n",
    "    topn=1\n",
    ")\n",
    "print(\"Government - Country + State =\", result[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0230d3e",
   "metadata": {},
   "source": [
    "<h1>Task 2. Model Comparison and Analysis<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd560143",
   "metadata": {},
   "source": [
    "1. Comparing Skip-gram, Skip-gram negative sampling, GloVe models on training loss, training time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f80d7a4",
   "metadata": {},
   "source": [
    "| Model                         | Training Loss (Epoch 5000) | Training Time per Epoch |\n",
    "|------------------------------|----------------------------|--------------------------|\n",
    "| Skip-gram (Full Softmax)     | 7.77                       | 162.14 ms                |\n",
    "| Skip-gram (Negative Sampling)| 10.85                      | 179.13 ms               |\n",
    "| GloVe                        | 0.13                      | 7.68 ms             |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e87599",
   "metadata": {},
   "source": [
    "The Skip-gram model without negative sampling shows relatively high training loss and inconsistent training time because it uses a full softmax over the entire vocabulary at each update, making it computationally expensive and less scalable. Skip-gram with negative sampling significantly improves efficiency by replacing the full softmax with a small number of negative samples, resulting in faster training and a steadily decreasing loss, although its loss values are not directly comparable due to a different objective function. GloVe achieves the lowest training loss and the fastest training time per epoch because it optimizes a global co-occurrence-based weighted least squares objective, allowing faster convergence once the co-occurrence matrix is constructed. Overall, Skip-gram with negative sampling provides the best balance between efficiency and representation quality, while GloVe is the most computationally efficient during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92f2b47",
   "metadata": {},
   "source": [
    "2. Using Word analogies dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "35f85ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of semantic analogies: 506\n",
      "Sample: [['athens', 'greece', 'baghdad', 'iraq'], ['athens', 'greece', 'bangkok', 'thailand'], ['athens', 'greece', 'beijing', 'china']]\n"
     ]
    }
   ],
   "source": [
    "# Load semantic analogy dataset\n",
    "\n",
    "semantic_dataset = []\n",
    "\n",
    "with open(\"capital-common-countries.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    words = line.strip().lower().split()\n",
    "    if len(words) == 4:\n",
    "        semantic_dataset.append([words[0], words[1], words[2], words[3]])\n",
    "\n",
    "print(\"Number of semantic analogies:\", len(semantic_dataset))\n",
    "print(\"Sample:\", semantic_dataset[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "228c086f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of syntactic analogies: 1560\n",
      "Sample: [['dancing', 'danced', 'decreasing', 'decreased'], ['dancing', 'danced', 'describing', 'described'], ['dancing', 'danced', 'enhancing', 'enhanced']]\n"
     ]
    }
   ],
   "source": [
    "# Load syntactic analogy dataset (past tense)\n",
    "\n",
    "past_tense_dataset = []\n",
    "\n",
    "with open(\"gram7-past-tense.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    words = line.strip().lower().split()\n",
    "    if len(words) == 4:\n",
    "        past_tense_dataset.append([words[0], words[1], words[2], words[3]])\n",
    "\n",
    "print(\"Number of syntactic analogies:\", len(past_tense_dataset))\n",
    "print(\"Sample:\", past_tense_dataset[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26400f84",
   "metadata": {},
   "source": [
    "Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8b88d873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_analogy(a, b, c, embed_fn, vocab_dict):\n",
    "    \"\"\"\n",
    "    Solve analogy: a : b :: c : ?\n",
    "    using vector arithmetic and cosine similarity\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve embeddings\n",
    "    va = np.asarray(embed_fn(a))\n",
    "    vb = np.asarray(embed_fn(b))\n",
    "    vc = np.asarray(embed_fn(c))\n",
    "\n",
    "    # Vector arithmetic: vb - va + vc\n",
    "    target_vec = vb - va + vc\n",
    "\n",
    "    best_match = None\n",
    "    highest_score = -1.0\n",
    "\n",
    "    for candidate in vocab_dict.keys():\n",
    "        # Skip original words\n",
    "        if candidate in {a, b, c}:\n",
    "            continue\n",
    "\n",
    "        candidate_vec = np.asarray(embed_fn(candidate))\n",
    "        score = cos_sim(target_vec, candidate_vec)\n",
    "\n",
    "        if score > highest_score:\n",
    "            highest_score = score\n",
    "            best_match = candidate\n",
    "\n",
    "    return best_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "793246a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy_accuracy(analogy_list, embed_fn, vocab_dict):\n",
    "    \"\"\"\n",
    "    Compute top-1 accuracy on a list of word analogies\n",
    "    \"\"\"\n",
    "\n",
    "    correct_predictions = 0\n",
    "    evaluated = 0\n",
    "\n",
    "    for a, b, c, d in analogy_list:\n",
    "\n",
    "        # Skip OOV cases\n",
    "        if not all(word in vocab_dict for word in [a, b, c, d]):\n",
    "            continue\n",
    "\n",
    "        predicted = solve_analogy(a, b, c, embed_fn, vocab_dict)\n",
    "\n",
    "        if predicted == d:\n",
    "            correct_predictions += 1\n",
    "\n",
    "        evaluated += 1\n",
    "\n",
    "    if evaluated == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return correct_predictions / evaluated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8333e6f7",
   "metadata": {},
   "source": [
    "Syntactic Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "53da9c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syntactic Accuracy\n",
      "------------------------------\n",
      "Syntactic Accuracy - Skip-gram: 0.00%\n",
      "Syntactic Accuracy - Skip-gram + Negative Sampling: 0.00%\n",
      "Syntactic Accuracy - GloVe: 0.00%\n"
     ]
    }
   ],
   "source": [
    "print(\"Syntactic Accuracy\\n\" + \"-\" * 30)\n",
    "\n",
    "models = [\n",
    "    (\"Skip-gram\", get_embed_skip_gram),\n",
    "    (\"Skip-gram + Negative Sampling\", get_embed_neg_sample),\n",
    "    (\"GloVe\", get_embed_glove),\n",
    "]\n",
    "\n",
    "for model_name, embed_fn in models:\n",
    "    acc = analogy_accuracy(past_tense_dataset, embed_fn, word2index)\n",
    "    print(f\"Syntactic Accuracy - {model_name}: {acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e2e2abe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syntactic Accuracy - Gensim (pretrained): 55.45%\n"
     ]
    }
   ],
   "source": [
    "# Create gensim-compatible syntactic file\n",
    "with open(\"gram7-past-tense.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "with open(\"gram7-past-tense_gensim.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\": gram7-past-tense\\n\")\n",
    "    for line in lines:\n",
    "        f.write(line)\n",
    "\n",
    "gensim_acc = model_genism.evaluate_word_analogies(\n",
    "    \"gram7-past-tense_gensim.txt\"\n",
    ")[0]\n",
    "\n",
    "print(f\"Syntactic Accuracy - Gensim (pretrained): {gensim_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6c8e5e",
   "metadata": {},
   "source": [
    "Semantic Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "25663bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Accuracy\n",
      "------------------------------\n",
      "Semantic Accuracy - Skip-gram: 0.00%\n",
      "Semantic Accuracy - Skip-gram + Negative Sampling: 0.00%\n",
      "Semantic Accuracy - GloVe: 0.00%\n"
     ]
    }
   ],
   "source": [
    "print(\"Semantic Accuracy\\n\" + \"-\" * 30)\n",
    "\n",
    "models = [\n",
    "    (\"Skip-gram\", get_embed_skip_gram),\n",
    "    (\"Skip-gram + Negative Sampling\", get_embed_neg_sample),\n",
    "    (\"GloVe\", get_embed_glove),\n",
    "]\n",
    "\n",
    "for model_name, embed_fn in models:\n",
    "    acc = analogy_accuracy(semantic_dataset, embed_fn, word2index)\n",
    "    print(f\"Semantic Accuracy - {model_name}: {acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a2eb9567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Accuracy - Gensim (pretrained): 93.87%\n"
     ]
    }
   ],
   "source": [
    "# Create gensim-compatible semantic file\n",
    "with open(\"capital-common-countries.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "with open(\"capital-common-countries_gensim.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\": capital-common-countries\\n\")\n",
    "    for line in lines:\n",
    "        f.write(line)\n",
    "\n",
    "gensim_acc = model_genism.evaluate_word_analogies(\n",
    "    \"capital-common-countries_gensim.txt\"\n",
    ")[0]\n",
    "\n",
    "print(f\"Semantic Accuracy - Gensim (pretrained): {gensim_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe4c1fc",
   "metadata": {},
   "source": [
    "| Model                    | Window Size | Training Loss (Epoch 5000) | Training Time (sec) | Syntactic Accuracy (%) | Semantic Accuracy (%) |\n",
    "|--------------------------|-------------|-----------------------------|---------------------|------------------------|-----------------------|\n",
    "| Skip-gram (Full Softmax) | 2           | 7.77                        | 0.162               | 0.0                    | 0.0                   |\n",
    "| Skip-gram (Neg Sampling) | 2           | 10.85                       | 0.179               | 0.0                    | 0.0                   |\n",
    "| GloVe                    | 2           | 0.13                        | 0.078               | 0.0                    | 0.0                   |\n",
    "| GloVe (Gensim Pretrained)|            |                            |                    | 55.45                  | 93.87                 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f557b9bf",
   "metadata": {},
   "source": [
    "<h1>Similarity Dataset<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afbe2eb",
   "metadata": {},
   "source": [
    "Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4083163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Define the column names\n",
    "columns = ['Word 1', 'Word 2', 'Similarity Index']\n",
    "df = pd.read_csv('wordsim_relatedness_goldstandard.txt', sep='\\t', header=None, names=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b02fa349",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    word_1 = row['Word 1']\n",
    "    word_2 = row['Word 2']\n",
    "\n",
    "    try:\n",
    "        neg_samp_1_embed    = get_embed_neg_sample(word_1)\n",
    "        neg_samp_2_embed    = get_embed_neg_sample(word_2)\n",
    "        skip_gram_1_embed  = get_embed_skip_gram(word_1)\n",
    "        skip_gram_2_embed    = get_embed_skip_gram(word_2)\n",
    "        glove_1_embed       = get_embed_glove(word_1)\n",
    "        glove_2_embed        = get_embed_glove(word_2)\n",
    "\n",
    "    except KeyError:\n",
    "        # Replacing missing embeddings with the embedding of '<UNK>'\n",
    "        neg_samp_1_embed    = get_embed_neg_sample('<UNK>')\n",
    "        neg_samp_2_embed    = get_embed_neg_sample('<UNK>')\n",
    "        skip_gram_1_embed   = get_embed_skip_gram('<UNK>')\n",
    "        skip_gram_2_embed   = get_embed_skip_gram('<UNK>')\n",
    "        glove_1_embed       = get_embed_glove('<UNK>')\n",
    "        glove_2_embed       = get_embed_glove('<UNK>')\n",
    "\n",
    "    # Computing dot product\n",
    "    df.at[index, 'dot_product_neg_samp'] = np.dot(neg_samp_1_embed, neg_samp_2_embed)\n",
    "    df.at[index, 'dot_product_skip_gram'] = np.dot(skip_gram_1_embed, skip_gram_2_embed)\n",
    "    df.at[index, 'dot_product_glove'] = np.dot(glove_1_embed, glove_2_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b3e82689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Similarity Index</th>\n",
       "      <th>dot_product_neg_samp</th>\n",
       "      <th>dot_product_skip_gram</th>\n",
       "      <th>dot_product_glove</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>computer</td>\n",
       "      <td>keyboard</td>\n",
       "      <td>7.62</td>\n",
       "      <td>35.665695</td>\n",
       "      <td>0.083752</td>\n",
       "      <td>1.889597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jerusalem</td>\n",
       "      <td>Israel</td>\n",
       "      <td>8.46</td>\n",
       "      <td>35.665695</td>\n",
       "      <td>0.083752</td>\n",
       "      <td>1.889597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>planet</td>\n",
       "      <td>galaxy</td>\n",
       "      <td>8.11</td>\n",
       "      <td>35.665695</td>\n",
       "      <td>0.083752</td>\n",
       "      <td>1.889597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>canyon</td>\n",
       "      <td>landscape</td>\n",
       "      <td>7.53</td>\n",
       "      <td>35.665695</td>\n",
       "      <td>0.083752</td>\n",
       "      <td>1.889597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OPEC</td>\n",
       "      <td>country</td>\n",
       "      <td>5.63</td>\n",
       "      <td>-0.913162</td>\n",
       "      <td>0.189415</td>\n",
       "      <td>-0.052364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>day</td>\n",
       "      <td>summer</td>\n",
       "      <td>3.94</td>\n",
       "      <td>-8.107746</td>\n",
       "      <td>0.664206</td>\n",
       "      <td>-0.390478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>day</td>\n",
       "      <td>dawn</td>\n",
       "      <td>7.53</td>\n",
       "      <td>6.027970</td>\n",
       "      <td>0.254585</td>\n",
       "      <td>1.390960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>country</td>\n",
       "      <td>citizen</td>\n",
       "      <td>7.31</td>\n",
       "      <td>-0.913162</td>\n",
       "      <td>0.189415</td>\n",
       "      <td>-0.052364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>planet</td>\n",
       "      <td>people</td>\n",
       "      <td>5.75</td>\n",
       "      <td>1.996283</td>\n",
       "      <td>0.137011</td>\n",
       "      <td>-0.110307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>environment</td>\n",
       "      <td>ecology</td>\n",
       "      <td>8.81</td>\n",
       "      <td>35.665695</td>\n",
       "      <td>0.083752</td>\n",
       "      <td>1.889597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Maradona</td>\n",
       "      <td>football</td>\n",
       "      <td>8.62</td>\n",
       "      <td>-0.648104</td>\n",
       "      <td>-0.228602</td>\n",
       "      <td>0.179088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>OPEC</td>\n",
       "      <td>oil</td>\n",
       "      <td>8.59</td>\n",
       "      <td>35.665695</td>\n",
       "      <td>0.083752</td>\n",
       "      <td>1.889597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>money</td>\n",
       "      <td>bank</td>\n",
       "      <td>8.50</td>\n",
       "      <td>8.641552</td>\n",
       "      <td>0.098085</td>\n",
       "      <td>0.377020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>computer</td>\n",
       "      <td>software</td>\n",
       "      <td>8.50</td>\n",
       "      <td>35.665695</td>\n",
       "      <td>0.083752</td>\n",
       "      <td>1.889597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>law</td>\n",
       "      <td>lawyer</td>\n",
       "      <td>8.38</td>\n",
       "      <td>3.423678</td>\n",
       "      <td>0.018705</td>\n",
       "      <td>-0.236578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word 1     Word 2  Similarity Index  dot_product_neg_samp  \\\n",
       "0      computer   keyboard              7.62             35.665695   \n",
       "1     Jerusalem     Israel              8.46             35.665695   \n",
       "2        planet     galaxy              8.11             35.665695   \n",
       "3        canyon  landscape              7.53             35.665695   \n",
       "4          OPEC    country              5.63             -0.913162   \n",
       "5           day     summer              3.94             -8.107746   \n",
       "6           day       dawn              7.53              6.027970   \n",
       "7       country    citizen              7.31             -0.913162   \n",
       "8        planet     people              5.75              1.996283   \n",
       "9   environment    ecology              8.81             35.665695   \n",
       "10     Maradona   football              8.62             -0.648104   \n",
       "11         OPEC        oil              8.59             35.665695   \n",
       "12        money       bank              8.50              8.641552   \n",
       "13     computer   software              8.50             35.665695   \n",
       "14          law     lawyer              8.38              3.423678   \n",
       "\n",
       "    dot_product_skip_gram  dot_product_glove  \n",
       "0                0.083752           1.889597  \n",
       "1                0.083752           1.889597  \n",
       "2                0.083752           1.889597  \n",
       "3                0.083752           1.889597  \n",
       "4                0.189415          -0.052364  \n",
       "5                0.664206          -0.390478  \n",
       "6                0.254585           1.390960  \n",
       "7                0.189415          -0.052364  \n",
       "8                0.137011          -0.110307  \n",
       "9                0.083752           1.889597  \n",
       "10              -0.228602           0.179088  \n",
       "11               0.083752           1.889597  \n",
       "12               0.098085           0.377020  \n",
       "13               0.083752           1.889597  \n",
       "14               0.018705          -0.236578  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2a87c001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman Correlation (Skip-gram): 0.0396\n",
      "Spearman Correlation (Neg Sampling): 0.0448\n",
      "Spearman Correlation (GloVe): 0.0744\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Spearman correlation \n",
    "sg_corr, _ = spearmanr(df['dot_product_skip_gram'], df['Similarity Index'])\n",
    "neg_corr, _ = spearmanr(df['dot_product_neg_samp'], df['Similarity Index'])\n",
    "glove_corr, _ = spearmanr(df['dot_product_glove'], df['Similarity Index'])\n",
    "\n",
    "print(f\"Spearman Correlation (Skip-gram): {sg_corr:.4f}\")\n",
    "print(f\"Spearman Correlation (Neg Sampling): {neg_corr:.4f}\")\n",
    "print(f\"Spearman Correlation (GloVe): {glove_corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c5cce2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman Correlation (Glove genism): 0.50\n"
     ]
    }
   ],
   "source": [
    "correlation_coefficient = model_genism.evaluate_word_pairs('wordsim_relatedness_goldstandard.txt')\n",
    "print(f\"Spearman Correlation (Glove genism): {correlation_coefficient[1][0]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "08bf395a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true: 5.29\n"
     ]
    }
   ],
   "source": [
    "# Mean human similarity (required by assignment)\n",
    "y_true = df['Similarity Index'].mean()\n",
    "print(f\"y_true: {y_true:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db73f28f",
   "metadata": {},
   "source": [
    "| Metric / Model              | Skip-gram | Skip-gram (Neg) | GloVe  | GloVe (Gensim) | y_true |\n",
    "|-----------------------------|----------:|----------------:|-------:|---------------:|-------:|\n",
    "| MSE  | 0.0396    | 0.0448          | 0.0744 | 0.50           | 5.29   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f2190ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built embedding dicts: skipgram=2947 neg=2947 glove=2947\n"
     ]
    }
   ],
   "source": [
    "def build_embedding_dict_skipgram_fullsoftmax(model, vocab, word2index, device):\n",
    "    \"\"\"\n",
    "    For your Skip-gram (full softmax) model:\n",
    "      - model.embedding_center\n",
    "      - model.embedding_outside\n",
    "    Saves averaged embedding = (v + u)/2 for each word.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    emb_dict = {}\n",
    "    unk = word2index[\"<UNK>\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for w in vocab:\n",
    "            idx = word2index.get(w, unk)\n",
    "            t = torch.LongTensor([idx]).to(device)\n",
    "\n",
    "            v = model.embedding_center(t)   # [1, D]\n",
    "            u = model.embedding_outside(t)  # [1, D]\n",
    "            emb = (v + u) / 2.0             # [1, D]\n",
    "\n",
    "            emb_dict[w] = emb.squeeze(0).cpu().numpy()\n",
    "\n",
    "    return emb_dict\n",
    "\n",
    "\n",
    "def build_embedding_dict_neg_sampling(model_neg, vocab, word2index, device):\n",
    "    \"\"\"\n",
    "    For your Negative Sampling model:\n",
    "      - model_neg.embedding_v\n",
    "      - model_neg.embedding_u\n",
    "    Saves averaged embedding = (v + u)/2 for each word.\n",
    "    \"\"\"\n",
    "    model_neg.eval()\n",
    "    emb_dict = {}\n",
    "    unk = word2index[\"<UNK>\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for w in vocab:\n",
    "            idx = word2index.get(w, unk)\n",
    "            t = torch.LongTensor([idx]).to(device)\n",
    "\n",
    "            v = model_neg.embedding_v(t)  # [1, D]\n",
    "            u = model_neg.embedding_u(t)  # [1, D]\n",
    "            emb = (v + u) / 2.0           # [1, D]\n",
    "\n",
    "            emb_dict[w] = emb.squeeze(0).cpu().numpy()\n",
    "\n",
    "    return emb_dict\n",
    "\n",
    "\n",
    "def build_embedding_dict_glove(model_glove, vocab, word2index, device):\n",
    "    \"\"\"\n",
    "    For your GloVe model:\n",
    "      - model_glove.embedding_v\n",
    "      - model_glove.embedding_u\n",
    "    Saves averaged embedding = (v + u)/2 for each word.\n",
    "    \"\"\"\n",
    "    model_glove.eval()\n",
    "    emb_dict = {}\n",
    "    unk = word2index[\"<UNK>\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for w in vocab:\n",
    "            idx = word2index.get(w, unk)\n",
    "            t = torch.LongTensor([idx]).to(device)\n",
    "\n",
    "            v = model_glove.embedding_v(t)  # [1, D]\n",
    "            u = model_glove.embedding_u(t)  # [1, D]\n",
    "            emb = (v + u) / 2.0             # [1, D]\n",
    "\n",
    "            emb_dict[w] = emb.squeeze(0).cpu().numpy()\n",
    "\n",
    "    return emb_dict\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Build embedding dictionaries (matches your variable names)\n",
    "# -------------------------\n",
    "\n",
    "embed_skipgram = build_embedding_dict_skipgram_fullsoftmax(model, vocab, word2index, device)\n",
    "embed_neg      = build_embedding_dict_neg_sampling(model_neg, vocab, word2index, device)\n",
    "embed_glove    = build_embedding_dict_glove(model_glove, vocab, word2index, device)\n",
    "\n",
    "print(\"Built embedding dicts:\",\n",
    "      f\"skipgram={len(embed_skipgram)}\",\n",
    "      f\"neg={len(embed_neg)}\",\n",
    "      f\"glove={len(embed_glove)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "79eadd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved all embedding pickles to ./model/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# -------------------------\n",
    "# Save for your web app\n",
    "# -------------------------\n",
    "\n",
    "os.makedirs(\"model\", exist_ok=True)\n",
    "\n",
    "# Save gensim pretrained model (optional baseline)\n",
    "with open(\"model/model_gensim.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_genism, f)\n",
    "\n",
    "# Save embedding dictionaries (fast lookup in web app)\n",
    "with open(\"model/embed_skipgram.pkl\", \"wb\") as f:\n",
    "    pickle.dump(embed_skipgram, f)\n",
    "\n",
    "with open(\"model/embed_skipgram_neg.pkl\", \"wb\") as f:\n",
    "    pickle.dump(embed_neg, f)\n",
    "\n",
    "with open(\"model/embed_glove.pkl\", \"wb\") as f:\n",
    "    pickle.dump(embed_glove, f)\n",
    "\n",
    "print(\"Saved all embedding pickles to ./model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd7702d",
   "metadata": {},
   "source": [
    "<h1>Conclusion<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a297bc87",
   "metadata": {},
   "source": [
    "In this assignment, Skip-gram (full softmax), Skip-gram with negative sampling, and GloVe models were implemented and evaluated using the news category of the Brown corpus. Skip-gram with full softmax showed higher training loss and inconsistent training time due to the computational cost of computing a full softmax over the vocabulary, while negative sampling significantly improved efficiency by approximating this objective. GloVe achieved the lowest training loss and fastest training time per epoch due to its global co-occurrencebased optimization. However, all models trained from scratch achieved 0% accuracy on both syntactic and semantic analogy tasks, as well as very low correlation with human similarity judgments, mainly due to the limited corpus size and vocabulary coverage. In contrast, the pretrained GloVe model from Gensim performed substantially better, achieving high syntactic and semantic analogy accuracy and a strong Spearman correlation with human similarity scores. Overall, the results highlight that while negative sampling and GloVe improve computational efficiency, large-scale training data is essential for learning meaningful semantic representations, making pretrained embeddings more suitable for real-world NLP applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd4f884",
   "metadata": {},
   "source": [
    "<h1>Screenshots of dash app<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa81ee30",
   "metadata": {},
   "source": [
    "![Dash App Screenshot](images/dash_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c31e76",
   "metadata": {},
   "source": [
    "![Dash App Screenshot](images/dash_2.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Machine_Learning (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
